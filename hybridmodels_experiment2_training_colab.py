# -*- coding: utf-8 -*-
"""HybridModels_Experiment2_Training_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u3YgTu2tsnw8vuOpW7tEXx2KtehK28tI
"""

# Install required packages
!pip install torch torchvision tqdm pillow matplotlib seaborn scikit-learn xgboost -q

print("âœ“ All packages installed!")

# Check GPU
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

from google.colab import drive
drive.mount('/content/drive')

# Set your dataset path
DATASET_DIR = '/content/drive/MyDrive/scenario2_dataset'

print(f"Dataset directory: {DATASET_DIR}")

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler
from torchvision import transforms
from PIL import Image

import numpy as np
import os
import time
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import warnings
warnings.filterwarnings('ignore')

print(" Libraries imported!")

class PlantDiseaseDataset(Dataset):

    def __init__(self, root_dir, split, transform=None, class_to_idx=None):

        self.root_dir = root_dir
        self.split = split
        self.transform = transform

        # Get split folder
        split_dir = os.path.join(root_dir, split)

        if not os.path.exists(split_dir):
            raise RuntimeError(f"Split folder not found: {split_dir}")

        # Get all class folders
        self.classes = sorted([d for d in os.listdir(split_dir)
                              if os.path.isdir(os.path.join(split_dir, d))])

        # Create class to index mapping
        if class_to_idx is None:
            self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}
        else:
            self.class_to_idx = class_to_idx

        # Collect all image paths
        self.samples = []

        for class_name in self.classes:
            class_folder = os.path.join(split_dir, class_name)
            class_idx = self.class_to_idx[class_name]

            # Get all images
            for img_name in os.listdir(class_folder):
                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):
                    img_path = os.path.join(class_folder, img_name)
                    self.samples.append((img_path, class_idx))

        print(f"Loaded {len(self.samples)} images from {len(self.classes)} classes ({split} split)")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        image = Image.open(img_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image, label

print("âœ“ Dataset class defined!")

# training augmentation for small dataset
train_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(p=0.5), # Essential for lateral variation

    # Reduced Aggression:
    transforms.RandomRotation(degrees=15), # Reduced from 45 degrees
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05), # Reduced factors

    transforms.ToTensor(),

    # Reduced RandomErasing
    transforms.RandomErasing(p=0.1, scale=(0.02, 0.1)), # Reduced probability and max size

    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Test transforms (no change)
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

print("âœ“ Stabilized augmentation transforms defined!")

# Load training data
train_dataset = PlantDiseaseDataset(
    root_dir=DATASET_DIR,
    split='train',
    transform=train_transform
)

# Load test data
test_dataset = PlantDiseaseDataset(
    root_dir=DATASET_DIR,
    split='test',
    transform=test_transform,
    class_to_idx=train_dataset.class_to_idx
)

num_classes = len(train_dataset.classes)
class_names = train_dataset.classes

print(f"\n{'='*60}")
print(f"Dataset Summary:")
print(f"{'='*60}")
print(f"Number of classes: {num_classes}")
print(f"Train samples: {len(train_dataset)} (will be split into 5 folds)")
print(f"Test samples: {len(test_dataset)}")
print(f"{'='*60}")

# 1. Extract labels (class indices) from the training dataset
train_labels = [sample[1] for sample in train_dataset.samples]

# 2. Calculate the raw count for each of the 12 classes
class_counts = np.bincount(train_labels)

# 3. Calculate Inverse Frequency Weights
# Weight = Total Samples / Class Count
total_samples = len(train_labels)
class_weights = total_samples / class_counts
num_classes = len(class_counts)
class_weights = class_weights / class_weights.sum() * num_classes

# 4. Convert to PyTorch Tensor and move to the device (CPU/GPU)
class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)

print(f"Total Samples: {total_samples}")
print(f"Class Counts: {class_counts}")
print(f"Calculated Class Weights (Higher = Rarer): {class_weights_tensor.cpu().numpy().round(2)}")
print("âœ“ Class weights calculated and ready for the loss function.")

# VGG13 Base Architecture
class VGG13Base(nn.Module):
    def __init__(self):
        super(VGG13Base, self).__init__()

        # Block 1
        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Block 2
        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Block 3
        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Block 4
        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Block 5
        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)

        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = F.relu(self.conv1_1(x))
        x = F.relu(self.conv1_2(x))
        x = self.pool1(x)

        x = F.relu(self.conv2_1(x))
        x = F.relu(self.conv2_2(x))
        x = self.pool2(x)

        x = F.relu(self.conv3_1(x))
        x = F.relu(self.conv3_2(x))
        x = F.relu(self.conv3_3(x))
        x = self.pool3(x)

        x = F.relu(self.conv4_1(x))
        x = F.relu(self.conv4_2(x))
        x = F.relu(self.conv4_3(x))
        x = self.pool4(x)

        x = F.relu(self.conv5_1(x))
        x = F.relu(self.conv5_2(x))
        x = F.relu(self.conv5_3(x))
        x = self.pool5(x)

        return x

print("VGG13 Base defined!")
# CBAM Attention Module
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        b, c, _, _ = x.size()
        avg_out = self.fc(self.avg_pool(x).view(b, c))
        max_out = self.fc(self.max_pool(x).view(b, c))
        out = self.sigmoid(avg_out + max_out).view(b, c, 1, 1)
        return x * out.expand_as(x)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x_cat = torch.cat([avg_out, max_out], dim=1)
        out = self.sigmoid(self.conv(x_cat))
        return x * out

class CBAM(nn.Module):
    def __init__(self, in_channels, reduction=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttention(in_channels, reduction)
        self.spatial_attention = SpatialAttention(kernel_size)

    def forward(self, x):
        x = self.channel_attention(x)
        x = self.spatial_attention(x)
        return x

# Model 1: VGG13 + Attention
class VGG13_Attention(nn.Module):
    def __init__(self, num_classes=12, dropout=0.5):
        super(VGG13_Attention, self).__init__()
        self.vgg_base = VGG13Base()
        self.attention = CBAM(in_channels=512, reduction=16, kernel_size=7)
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.dropout = nn.Dropout(p=dropout)
        self.fc = nn.Linear(512, num_classes)
        nn.init.kaiming_normal_(self.fc.weight)
        nn.init.constant_(self.fc.bias, 0)

    def forward(self, x):
        x = self.vgg_base(x)
        x = self.attention(x)
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        return x

print("VGG13 + Attention defined!")
# Model 2: VGG13 + XGBoost (Feature Extractor)
class VGG13_FeatureExtractor(nn.Module):
    def __init__(self, num_classes=12, dropout=0.5):
        super(VGG13_FeatureExtractor, self).__init__()
        self.vgg_base = VGG13Base()
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.dropout = nn.Dropout(p=dropout)
        self.fc = nn.Linear(512, num_classes)
        nn.init.kaiming_normal_(self.fc.weight)
        nn.init.constant_(self.fc.bias, 0)

    def forward(self, x):
        x = self.vgg_base(x)
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        return x

    def extract_features(self, x):
        with torch.no_grad():
            x = self.vgg_base(x)
            x = self.global_pool(x)
            x = x.view(x.size(0), -1)
        return x

print("VGG13 + XGBoost (Feature Extractor) defined!")
# Model 3: VGG13 + Lightweight ViT
class PatchEmbedding(nn.Module):
    def __init__(self, in_channels=512, embed_dim=256):
        super(PatchEmbedding, self).__init__()
        self.projection = nn.Linear(in_channels, embed_dim)

    def forward(self, x):
        b, c, h, w = x.size()
        x = x.flatten(2).transpose(1, 2)
        x = self.projection(x)
        return x

class TransformerEncoderLayer(nn.Module):
    def __init__(self, embed_dim=256, num_heads=4, mlp_ratio=4, dropout=0.3):
        super(TransformerEncoderLayer, self).__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.norm2 = nn.LayerNorm(embed_dim)
        mlp_hidden_dim = int(embed_dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden_dim, embed_dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        x_norm = self.norm1(x)
        attn_out, _ = self.attention(x_norm, x_norm, x_norm)
        x = x + attn_out
        x = x + self.mlp(self.norm2(x))
        return x

class VGG13_ViT(nn.Module):
    def __init__(self, num_classes=12, embed_dim=256, num_heads=4, num_layers=2, mlp_ratio=4, dropout=0.3):
        super(VGG13_ViT, self).__init__()
        self.vgg_base = VGG13Base()
        self.patch_embed = PatchEmbedding(in_channels=512, embed_dim=embed_dim)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, 50, embed_dim))
        self.transformer_layers = nn.ModuleList([
            TransformerEncoderLayer(embed_dim, num_heads, mlp_ratio, dropout)
            for _ in range(num_layers)
        ])
        self.norm = nn.LayerNorm(embed_dim)
        self.head_dropout = nn.Dropout(p=dropout)
        self.head = nn.Linear(embed_dim, num_classes)
        self._initialize_weights()

    def _initialize_weights(self):
        nn.init.trunc_normal_(self.cls_token, std=0.02)
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.kaiming_normal_(self.head.weight)
        nn.init.constant_(self.head.bias, 0)

    def forward(self, x):
        batch_size = x.size(0)
        x = self.vgg_base(x)
        x = self.patch_embed(x)
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        x = x + self.pos_embed
        for layer in self.transformer_layers:
            x = layer(x)
        x = self.norm(x)
        cls_output = x[:, 0]
        x = self.head_dropout(cls_output)
        x = self.head(x)
        return x

print(" VGG13 + ViT defined!")
## 8ï¸âƒ£ Training Functions

def train_one_epoch_cv(model, train_loader, criterion, optimizer, device):
    """Train for one epoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    return running_loss / len(train_loader), 100. * correct / total

def validate_cv(model, val_loader, criterion, device):
    """Validate"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    return running_loss / len(val_loader), 100. * correct / total

print("Cross-validation training functions defined!")

# ============================================================================
# CONFIGURATION - CHANGE THIS TO TRAIN DIFFERENT MODELS
# ============================================================================

MODEL_NAME = 'attention'  # 'attention', 'xgboost', or 'vit'
NUM_EPOCHS = 100  # More epochs for small dataset
LEARNING_RATE = 0.001  # Lower LR for small dataset
PATIENCE = 30
N_FOLDS = 5  # 5-fold cross-validation
BATCH_SIZE = 64  # Smaller batch for small dataset

print(f"Training {MODEL_NAME.upper()} with {N_FOLDS}-Fold Cross-Validation")
print(f"Epochs: {NUM_EPOCHS}, LR: {LEARNING_RATE}, Patience: {PATIENCE}")

# Setup K-Fold
kfold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)

# Track results across folds
fold_results = []
all_fold_histories = []

print(f"\n{'='*70}")
print(f"Starting {N_FOLDS}-Fold Cross-Validation")
print(f"{'='*70}\n")

overall_start_time = time.time()


# K-Fold Cross-Validation Loop
for fold, (train_idx, val_idx) in enumerate(kfold.split(range(len(train_dataset)), train_labels)):
    print(f"\n{'='*70}")
    print(f"FOLD {fold + 1}/{N_FOLDS}")
    print(f"{'='*70}")

    # Create data samplers
    train_sampler = SubsetRandomSampler(train_idx)
    val_sampler = SubsetRandomSampler(val_idx)

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        sampler=train_sampler,
        num_workers=2,
        pin_memory=True
    )

    val_loader = DataLoader(
        train_dataset,  # Use same dataset but different indices
        batch_size=BATCH_SIZE,
        sampler=val_sampler,
        num_workers=2,
        pin_memory=True
    )

    print(f"Train samples: {len(train_idx)}, Val samples: {len(val_idx)}")

    # Create fresh model for each fold
    if MODEL_NAME == 'attention':
        model = VGG13_Attention(num_classes=num_classes, dropout=0.5).to(device)
        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4, nesterov=True)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)
    elif MODEL_NAME == 'xgboost':
        model = VGG13_FeatureExtractor(num_classes=num_classes, dropout=0.6).to(device)
        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=15)
    elif MODEL_NAME == 'vit':
        model = VGG13_ViT(num_classes=num_classes, embed_dim=256, num_heads=4, num_layers=2, mlp_ratio=4, dropout=0.3).to(device)
        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.05)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)

    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)

     # Training loop for this fold
    best_val_acc = 0.0
    patience_counter = 0
    fold_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    for epoch in range(1, NUM_EPOCHS + 1):
        # Train
        train_loss, train_acc = train_one_epoch_cv(model, train_loader, criterion, optimizer, device)

        # Validate
        val_loss, val_acc = validate_cv(model, val_loader, criterion, device)

        # Update scheduler
        if MODEL_NAME in ['attention', 'xgboost']:
            scheduler.step(val_acc)
        else:
            scheduler.step()

        # Save history
        fold_history['train_loss'].append(train_loss)
        fold_history['train_acc'].append(train_acc)
        fold_history['val_loss'].append(val_loss)
        fold_history['val_acc'].append(val_acc)


        print(f"Epoch {epoch}/{NUM_EPOCHS}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%")

        if epoch == 1 and fold == 0:
          print(f"\n{'='*50}")
          print("ðŸ” GRADIENT CHECK:")
          print(f"{'='*50}")

          # Check if gradients exist
          has_grad = False
          total_grad_norm = 0
          for name, param in model.named_parameters():
              if param.grad is not None:
                  has_grad = True
                  grad_norm = param.grad.norm().item()
                  total_grad_norm += grad_norm
                  if 'fc' in name or 'head' in name:  # Check final layer
                      print(f"{name}: grad_norm={grad_norm:.6f}")

          print(f"Total gradient norm: {total_grad_norm:.6f}")
          print(f"Gradients exist: {has_grad}")

          # Check weight changes
          print(f"\nChecking if weights actually updated...")
          print(f"Train loss: {train_loss:.4f}")
          print(f"Train acc: {train_acc:.2f}%")
          print(f"{'='*50}\n")

        # Save best model for this fold
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0

            # Save checkpoint
            torch.save({
                'model': model.state_dict(),
                'fold': fold,
                'best_val_acc': best_val_acc
            }, f'/content/{MODEL_NAME}_fold{fold+1}_best.pth')
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                print(f"\nEarly stopping at epoch {epoch}")
                break

    print(f"\nFold {fold + 1} Best Val Accuracy: {best_val_acc:.2f}%")
    fold_results.append(best_val_acc)
    all_fold_histories.append(fold_history)


overall_time = (time.time() - overall_start_time) / 60

# Print cross-validation summary
print(f"\n{'='*70}")
print(f"CROSS-VALIDATION SUMMARY")
print(f"{'='*70}")
for i, acc in enumerate(fold_results):
    print(f"Fold {i+1}: {acc:.2f}%")
print(f"{'-'*70}")
print(f"Mean CV Accuracy: {np.mean(fold_results):.2f}% \u00b1 {np.std(fold_results):.2f}%")
print(f"Total Training Time: {overall_time:.2f} minutes")
print(f"{'='*70}")

"""## ðŸ”Ÿ Test on Held-Out Test Set

**Use the best fold model for final testing**
"""

# ============================================================================
# COMPREHENSIVE EVALUATION WITH ALL METRICS
# ============================================================================

# Find best fold
best_fold_idx = np.argmax(fold_results)
best_fold_acc = fold_results[best_fold_idx]

print(f"Best fold: Fold {best_fold_idx + 1} with {best_fold_acc:.2f}% validation accuracy")

# Load best fold model
if MODEL_NAME == 'attention':
    final_model = VGG13_Attention(num_classes=num_classes, dropout=0.6).to(device)
elif MODEL_NAME == 'xgboost':
    final_model = VGG13_FeatureExtractor(num_classes=num_classes, dropout=0.6).to(device)
elif MODEL_NAME == 'vit':
    final_model = VGG13_ViT(num_classes=num_classes, dropout=0.4).to(device)

checkpoint = torch.load(f'/content/{MODEL_NAME}_fold{best_fold_idx+1}_best.pth')
final_model.load_state_dict(checkpoint['model'])
final_model.eval()

print(f"âœ“ Loaded best fold model")

# Create test loader
test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True
)

# Test with all metrics
print("\nTesting on held-out test set...")

inference_times = []
all_preds = []
all_labels = []
correct = 0
total = 0

with torch.no_grad():
    for images, labels in tqdm(test_loader, desc='Testing'):
        images, labels = images.to(device), labels.to(device)

        # Measure inference time
        start_time = time.time()
        outputs = final_model(images)
        inference_time = (time.time() - start_time) * 1000  # ms
        inference_times.append(inference_time)

        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate all metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

test_accuracy = 100. * correct / total
test_precision_macro = precision_score(all_labels, all_preds, average='macro') * 100
test_recall_macro = recall_score(all_labels, all_preds, average='macro') * 100
test_f1_macro = f1_score(all_labels, all_preds, average='macro') * 100
test_f1_weighted = f1_score(all_labels, all_preds, average='weighted') * 100

# Inference time metrics
avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
avg_inference_per_image = avg_inference_time / BATCH_SIZE

# Print comprehensive results
print(f"\n{'='*70}")
print(f"SCENARIO 2 COMPREHENSIVE RESULTS - {MODEL_NAME.upper()}")
print(f"{'='*70}")
print(f"\nðŸ“Š CROSS-VALIDATION RESULTS:")
print(f"  Fold 1:  {fold_results[0]:.2f}%")
print(f"  Fold 2:  {fold_results[1]:.2f}%")
print(f"  Fold 3:  {fold_results[2]:.2f}%")
print(f"  Fold 4:  {fold_results[3]:.2f}%")
print(f"  Fold 5:  {fold_results[4]:.2f}%")
print(f"  {'â”€'*50}")
print(f"  Mean CV Accuracy:  {np.mean(fold_results):.2f}% Â± {np.std(fold_results):.2f}%")
print(f"  Best Fold:         Fold {best_fold_idx + 1} ({best_fold_acc:.2f}%)")
print(f"\nðŸ“Š TEST SET METRICS:")
print(f"  Test Accuracy:      {test_accuracy:.2f}%")
print(f"\nðŸ“ˆ PRECISION & RECALL:")
print(f"  Precision (Macro):  {test_precision_macro:.2f}%")
print(f"  Recall (Macro):     {test_recall_macro:.2f}%")
print(f"\nðŸŽ¯ F1 SCORES:")
print(f"  Macro F1:           {test_f1_macro:.2f}%")
print(f"  Weighted F1:        {test_f1_weighted:.2f}%")
print(f"\nâ±ï¸ TIME METRICS:")
print(f"  Training Time:      {overall_time:.2f} minutes ({N_FOLDS} folds)")
print(f"  Avg Inference Time: {avg_inference_time:.2f} ms/batch ({avg_inference_per_image:.2f} ms/image)")
print(f"  Std Inference Time: {std_inference_time:.2f} ms/batch")
print(f"{'='*70}")

# Save metrics
metrics = {
    'model_name': MODEL_NAME,
    'scenario': 2,
    # Cross-validation
    'cv_fold_1': fold_results[0],
    'cv_fold_2': fold_results[1],
    'cv_fold_3': fold_results[2],
    'cv_fold_4': fold_results[3],
    'cv_fold_5': fold_results[4],
    'cv_mean': np.mean(fold_results),
    'cv_std': np.std(fold_results),
    'best_fold': best_fold_idx + 1,
    'best_fold_acc': best_fold_acc,
    # Test set metrics
    'test_accuracy': test_accuracy,
    'precision_macro': test_precision_macro,
    'recall_macro': test_recall_macro,
    'f1_macro': test_f1_macro,
    'f1_weighted': test_f1_weighted,
    # Time metrics
    'training_time_minutes': overall_time,
    'training_time_per_fold': overall_time / N_FOLDS,
    'inference_time_ms_per_batch': avg_inference_time,
    'inference_time_ms_per_image': avg_inference_per_image,
    'inference_time_std': std_inference_time,
    # Additional info
    'num_parameters': sum(p.numel() for p in final_model.parameters()),
    'batch_size': BATCH_SIZE,
    'n_folds': N_FOLDS
}

# Save as JSON
import json
with open(f'/content/{MODEL_NAME}_scenario2_metrics.json', 'w') as f:
    json.dump(metrics, f, indent=4)

print(f"\nâœ“ Metrics saved to {MODEL_NAME}_scenario2_metrics.json")

# Confusion Matrix
cm = confusion_matrix(all_labels, all_preds)

plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title(f'Confusion Matrix - {MODEL_NAME.upper()} (Scenario 2)', fontsize=14, fontweight='bold')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig(f'/content/{MODEL_NAME}_scenario2_confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ“ Confusion matrix saved!")

# ============================================================================
# CREATE COMPREHENSIVE METRICS VISUALIZATION
# ============================================================================

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Cross-validation results (already have this, enhance it)
ax1 = axes[0, 0]
folds = [f'F{i+1}' for i in range(N_FOLDS)]
bars1 = ax1.bar(folds, fold_results, color='skyblue', edgecolor='navy', linewidth=1.5)
ax1.axhline(y=np.mean(fold_results), color='red', linestyle='--', linewidth=2,
           label=f'Mean: {np.mean(fold_results):.2f}%')
ax1.set_ylabel('Validation Accuracy (%)', fontsize=11)
ax1.set_title(f'{N_FOLDS}-Fold CV Results', fontsize=12, fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3, axis='y')
for i, v in enumerate(fold_results):
    ax1.text(i, v + 0.5, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)

# 2. All accuracy metrics
ax2 = axes[0, 1]
metric_names = ['Accuracy', 'Precision', 'Recall', 'Macro F1', 'Weighted F1']
metric_values = [test_accuracy, test_precision_macro, test_recall_macro, test_f1_macro, test_f1_weighted]
bars2 = ax2.barh(metric_names, metric_values, color='#3498db')
ax2.set_xlabel('Score (%)', fontsize=11)
ax2.set_title(f'All Metrics (Test Set)', fontsize=12, fontweight='bold')
ax2.set_xlim([0, 100])
for i, bar in enumerate(bars2):
    width = bar.get_width()
    ax2.text(width, bar.get_y() + bar.get_height()/2.,
            f' {width:.1f}%', ha='left', va='center', fontweight='bold', fontsize=9)

# 3. CV vs Test accuracy
ax3 = axes[0, 2]
comparison = [np.mean(fold_results), test_accuracy]
labels_comp = ['CV Mean', 'Test']
bars3 = ax3.bar(labels_comp, comparison, color=['#2ecc71', '#e74c3c'])
ax3.set_ylabel('Accuracy (%)', fontsize=11)
ax3.set_title('CV vs Test Accuracy', fontsize=12, fontweight='bold')
ax3.set_ylim([0, 100])
for bar in bars3:
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)

# 4. Time metrics
ax4 = axes[1, 0]
times = [overall_time * 60, avg_inference_per_image / 1000]  # Convert to seconds
labels_time = [f'Training\n({overall_time:.1f} min)', f'Inference\n({avg_inference_per_image:.2f} ms)']
colors = ['#f39c12', '#9b59b6']
bars4 = ax4.bar(labels_time, times, color=colors)
ax4.set_ylabel('Time (seconds)', fontsize=11)
ax4.set_title('Time Metrics', fontsize=12, fontweight='bold')
ax4.set_yscale('log')
for bar in bars4:
    height = bar.get_height()
    if height > 60:
        text = f'{height/60:.1f} min'
    else:
        text = f'{height:.2f} s'
    ax4.text(bar.get_x() + bar.get_width()/2., height,
            text, ha='center', va='bottom', fontweight='bold', fontsize=9)

# 5. F1 comparison
ax5 = axes[1, 1]
f1_types = ['Macro F1', 'Weighted F1']
f1_values = [test_f1_macro, test_f1_weighted]
bars5 = ax5.bar(f1_types, f1_values, color=['#1abc9c', '#16a085'])
ax5.set_ylabel('F1 Score (%)', fontsize=11)
ax5.set_title('F1 Score Comparison', fontsize=12, fontweight='bold')
ax5.set_ylim([0, 100])
for bar in bars5:
    height = bar.get_height()
    ax5.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)

# 6. Summary table
ax6 = axes[1, 2]
ax6.axis('off')
summary_data = [
    ['CV Mean', f"{np.mean(fold_results):.2f}% Â± {np.std(fold_results):.2f}%"],
    ['Test Acc', f"{test_accuracy:.2f}%"],
    ['Macro F1', f"{test_f1_macro:.2f}%"],
    ['Weighted F1', f"{test_f1_weighted:.2f}%"],
    ['Training', f"{overall_time:.2f} min"],
    ['Inference', f"{avg_inference_per_image:.2f} ms/img"],
    ['Parameters', f"{metrics['num_parameters']:,}"]
]
table = ax6.table(cellText=summary_data, cellLoc='left', loc='center',
                 colWidths=[0.5, 0.5], bbox=[0, 0, 1, 1])
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1, 2)
for i in range(len(summary_data)):
    table[(i, 0)].set_facecolor('#ecf0f1')
    table[(i, 0)].set_text_props(weight='bold')
ax6.set_title('Summary Metrics', fontsize=12, fontweight='bold', pad=20)

plt.tight_layout()
plt.savefig(f'/content/{MODEL_NAME}_scenario2_comprehensive_metrics.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ“ Comprehensive metrics visualization saved!")

# Classification Report
print("\nClassification Report:")
print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))

# Plot CV results
fig, ax = plt.subplots(figsize=(10, 6))

folds = [f'Fold {i+1}' for i in range(N_FOLDS)]
ax.bar(folds, fold_results, color='skyblue', edgecolor='navy', linewidth=1.5)
ax.axhline(y=np.mean(fold_results), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(fold_results):.2f}%')
ax.set_ylabel('Validation Accuracy (%)', fontsize=12)
ax.set_title(f'{N_FOLDS}-Fold Cross-Validation Results - {MODEL_NAME.upper()}', fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3, axis='y')

# Add value labels on bars
for i, v in enumerate(fold_results):
    ax.text(i, v + 1, f'{v:.2f}%', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.savefig(f'/content/{MODEL_NAME}_scenario2_cv_results.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ“ CV results visualization saved!")

# Copy all results to Google Drive
!mkdir -p "/content/drive/MyDrive/scenario2_results"

# Copy best fold model
!cp /content/{MODEL_NAME}_fold{best_fold_idx+1}_best.pth "/content/drive/MyDrive/scenario2_results/"

# Copy visualizations
!cp /content/{MODEL_NAME}_scenario2_confusion_matrix.png "/content/drive/MyDrive/scenario2_results/"
!cp /content/{MODEL_NAME}_scenario2_cv_results.png "/content/drive/MyDrive/scenario2_results/"
!cp /content/{MODEL_NAME}_scenario2_comprehensive_metrics.png "/content/drive/MyDrive/scenario2_results/"

# Save metrics
!cp /content/{MODEL_NAME}_scenario2_metrics.json "/content/drive/MyDrive/scenario2_results/"

# Save CV summary
with open(f'/content/{MODEL_NAME}_scenario2_cv_summary.txt', 'w') as f:
    f.write(f"Scenario 2 Cross-Validation Results - {MODEL_NAME.upper()}\n")
    f.write("="*70 + "\n\n")
    f.write("CROSS-VALIDATION:\n")
    for i, acc in enumerate(fold_results):
        f.write(f"  Fold {i+1}: {acc:.2f}%\n")
    f.write(f"\n  Mean: {np.mean(fold_results):.2f}% Â± {np.std(fold_results):.2f}%\n")
    f.write(f"  Best: Fold {best_fold_idx+1} ({best_fold_acc:.2f}%)\n\n")
    f.write("TEST SET METRICS:\n")
    f.write(f"  Accuracy:        {test_accuracy:.2f}%\n")
    f.write(f"  Precision:       {test_precision_macro:.2f}%\n")
    f.write(f"  Recall:          {test_recall_macro:.2f}%\n")
    f.write(f"  Macro F1:        {test_f1_macro:.2f}%\n")
    f.write(f"  Weighted F1:     {test_f1_weighted:.2f}%\n\n")
    f.write("TIME:\n")
    f.write(f"  Training:        {overall_time:.2f} min\n")
    f.write(f"  Inference:       {avg_inference_per_image:.2f} ms/image\n")

!cp /content/{MODEL_NAME}_scenario2_cv_summary.txt "/content/drive/MyDrive/scenario2_results/"

print("âœ“ All results saved to Google Drive!")
print("  Location: /MyDrive/scenario2_results/")
print("\nðŸ“ Files saved:")
print(f"  - {MODEL_NAME}_fold{best_fold_idx+1}_best.pth")
print(f"  - {MODEL_NAME}_scenario2_confusion_matrix.png")
print(f"  - {MODEL_NAME}_scenario2_cv_results.png")
print(f"  - {MODEL_NAME}_scenario2_comprehensive_metrics.png")
print(f"  - {MODEL_NAME}_scenario2_metrics.json")
print(f"  - {MODEL_NAME}_scenario2_cv_summary.txt")

"""XGboost

"""

# ============================================================================
# CONFIGURATION - CHANGE THIS TO TRAIN DIFFERENT MODELS
# ============================================================================

MODEL_NAME = 'xgboost'  # 'attention', 'xgboost', or 'vit'
NUM_EPOCHS = 200  # More epochs for small dataset
LEARNING_RATE = 0.001  # Lower LR for small dataset
PATIENCE = 30
N_FOLDS = 5  # 5-fold cross-validation
BATCH_SIZE = 32  # Smaller batch for small dataset

print(f"Training {MODEL_NAME.upper()} with {N_FOLDS}-Fold Cross-Validation")
print(f"Epochs: {NUM_EPOCHS}, LR: {LEARNING_RATE}, Patience: {PATIENCE}")

# Setup K-Fold
kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)

# Track results across folds
fold_results = []
all_fold_histories = []

criterion = nn.CrossEntropyLoss()

print(f"\n{'='*70}")
print(f"Starting {N_FOLDS}-Fold Cross-Validation")
print(f"{'='*70}\n")

overall_start_time = time.time()

# K-Fold Cross-Validation Loop
for fold, (train_idx, val_idx) in enumerate(kfold.split(range(len(train_dataset)))):
    print(f"\n{'='*70}")
    print(f"FOLD {fold + 1}/{N_FOLDS}")
    print(f"{'='*70}")

    # Create data samplers
    train_sampler = SubsetRandomSampler(train_idx)
    val_sampler = SubsetRandomSampler(val_idx)

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        sampler=train_sampler,
        num_workers=2,
        pin_memory=True
    )

    val_loader = DataLoader(
        train_dataset,  # Use same dataset but different indices
        batch_size=BATCH_SIZE,
        sampler=val_sampler,
        num_workers=2,
        pin_memory=True
    )

    print(f"Train samples: {len(train_idx)}, Val samples: {len(val_idx)}")

    # Create fresh model for each fold
    if MODEL_NAME == 'attention':
        model = VGG13_Attention(num_classes=num_classes, dropout=0.6).to(device)
        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=15)
    elif MODEL_NAME == 'xgboost':
        model = VGG13_FeatureExtractor(num_classes=num_classes, dropout=0.6).to(device)
        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=15)
    elif MODEL_NAME == 'vit':
        model = VGG13_ViT(num_classes=num_classes, dropout=0.4).to(device)
        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.05)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)

    # Training loop for this fold
    best_val_acc = 0.0
    patience_counter = 0
    fold_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    for epoch in range(1, NUM_EPOCHS + 1):
        # Train
        train_loss, train_acc = train_one_epoch_cv(model, train_loader, criterion, optimizer, device)

        # Validate
        val_loss, val_acc = validate_cv(model, val_loader, criterion, device)

        # Update scheduler
        if MODEL_NAME in ['attention', 'xgboost']:
            scheduler.step(val_acc)
        else:
            scheduler.step()

        # Save history
        fold_history['train_loss'].append(train_loss)
        fold_history['train_acc'].append(train_acc)
        fold_history['val_loss'].append(val_loss)
        fold_history['val_acc'].append(val_acc)


        print(f"Epoch {epoch}/{NUM_EPOCHS}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%")

        # Save best model for this fold
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0

            # Save checkpoint
            torch.save({
                'model': model.state_dict(),
                'fold': fold,
                'best_val_acc': best_val_acc
            }, f'/content/{MODEL_NAME}_fold{fold+1}_best.pth')
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                print(f"\nEarly stopping at epoch {epoch}")
                break

    print(f"\nFold {fold + 1} Best Val Accuracy: {best_val_acc:.2f}%")
    fold_results.append(best_val_acc)
    all_fold_histories.append(fold_history)

overall_time = (time.time() - overall_start_time) / 60

# Print cross-validation summary
print(f"\n{'='*70}")
print(f"CROSS-VALIDATION SUMMARY")
print(f"{'='*70}")
for i, acc in enumerate(fold_results):
    print(f"Fold {i+1}: {acc:.2f}%")
print(f"{'-'*70}")
print(f"Mean CV Accuracy: {np.mean(fold_results):.2f}% Â± {np.std(fold_results):.2f}%")
print(f"Total Training Time: {overall_time:.2f} minutes")
print(f"{'='*70}")

# ============================================================================
# COMPREHENSIVE EVALUATION WITH ALL METRICS
# ============================================================================

# Find best fold
best_fold_idx = np.argmax(fold_results)
best_fold_acc = fold_results[best_fold_idx]

print(f"Best fold: Fold {best_fold_idx + 1} with {best_fold_acc:.2f}% validation accuracy")

# Load best fold model
if MODEL_NAME == 'attention':
    final_model = VGG13_Attention(num_classes=num_classes, dropout=0.6).to(device)
elif MODEL_NAME == 'xgboost':
    final_model = VGG13_FeatureExtractor(num_classes=num_classes, dropout=0.6).to(device)
elif MODEL_NAME == 'vit':
    final_model = VGG13_ViT(num_classes=num_classes, dropout=0.4).to(device)

checkpoint = torch.load(f'/content/{MODEL_NAME}_fold{best_fold_idx+1}_best.pth')
final_model.load_state_dict(checkpoint['model'])
final_model.eval()

print(f"âœ“ Loaded best fold model")

# Create test loader
test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True
)

# Test with all metrics
print("\nTesting on held-out test set...")

inference_times = []
all_preds = []
all_labels = []
correct = 0
total = 0

with torch.no_grad():
    for images, labels in tqdm(test_loader, desc='Testing'):
        images, labels = images.to(device), labels.to(device)

        # Measure inference time
        start_time = time.time()
        outputs = final_model(images)
        inference_time = (time.time() - start_time) * 1000  # ms
        inference_times.append(inference_time)

        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate all metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

test_accuracy = 100. * correct / total
test_precision_macro = precision_score(all_labels, all_preds, average='macro') * 100
test_recall_macro = recall_score(all_labels, all_preds, average='macro') * 100
test_f1_macro = f1_score(all_labels, all_preds, average='macro') * 100
test_f1_weighted = f1_score(all_labels, all_preds, average='weighted') * 100

# Inference time metrics
avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
avg_inference_per_image = avg_inference_time / BATCH_SIZE

# Print comprehensive results
print(f"\n{'='*70}")
print(f"SCENARIO 2 COMPREHENSIVE RESULTS - {MODEL_NAME.upper()}")
print(f"{'='*70}")
print(f"\nðŸ“Š CROSS-VALIDATION RESULTS:")
print(f"  Fold 1:  {fold_results[0]:.2f}%")
print(f"  Fold 2:  {fold_results[1]:.2f}%")
print(f"  Fold 3:  {fold_results[2]:.2f}%")
print(f"  Fold 4:  {fold_results[3]:.2f}%")
print(f"  Fold 5:  {fold_results[4]:.2f}%")
print(f"  {'â”€'*50}")
print(f"  Mean CV Accuracy:  {np.mean(fold_results):.2f}% Â± {np.std(fold_results):.2f}%")
print(f"  Best Fold:         Fold {best_fold_idx + 1} ({best_fold_acc:.2f}%)")
print(f"\nðŸ“Š TEST SET METRICS:")
print(f"  Test Accuracy:      {test_accuracy:.2f}%")
print(f"\nðŸ“ˆ PRECISION & RECALL:")
print(f"  Precision (Macro):  {test_precision_macro:.2f}%")
print(f"  Recall (Macro):     {test_recall_macro:.2f}%")
print(f"\nðŸŽ¯ F1 SCORES:")
print(f"  Macro F1:           {test_f1_macro:.2f}%")
print(f"  Weighted F1:        {test_f1_weighted:.2f}%")
print(f"\nâ±ï¸ TIME METRICS:")
print(f"  Training Time:      {overall_time:.2f} minutes ({N_FOLDS} folds)")
print(f"  Avg Inference Time: {avg_inference_time:.2f} ms/batch ({avg_inference_per_image:.2f} ms/image)")
print(f"  Std Inference Time: {std_inference_time:.2f} ms/batch")
print(f"{'='*70}")

# Save metrics
metrics = {
    'model_name': MODEL_NAME,
    'scenario': 2,
    # Cross-validation
    'cv_fold_1': fold_results[0],
    'cv_fold_2': fold_results[1],
    'cv_fold_3': fold_results[2],
    'cv_fold_4': fold_results[3],
    'cv_fold_5': fold_results[4],
    'cv_mean': np.mean(fold_results),
    'cv_std': np.std(fold_results),
    'best_fold': best_fold_idx + 1,
    'best_fold_acc': best_fold_acc,
    # Test set metrics
    'test_accuracy': test_accuracy,
    'precision_macro': test_precision_macro,
    'recall_macro': test_recall_macro,
    'f1_macro': test_f1_macro,
    'f1_weighted': test_f1_weighted,
    # Time metrics
    'training_time_minutes': overall_time,
    'training_time_per_fold': overall_time / N_FOLDS,
    'inference_time_ms_per_batch': avg_inference_time,
    'inference_time_ms_per_image': avg_inference_per_image,
    'inference_time_std': std_inference_time,
    # Additional info
    'num_parameters': sum(p.numel() for p in final_model.parameters()),
    'batch_size': BATCH_SIZE,
    'n_folds': N_FOLDS
}

# Save as JSON
import json
with open(f'/content/{MODEL_NAME}_scenario2_metrics.json', 'w') as f:
    json.dump(metrics, f, indent=4)

print(f"\nâœ“ Metrics saved to {MODEL_NAME}_scenario2_metrics.json")

# Confusion Matrix
cm = confusion_matrix(all_labels, all_preds)

plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title(f'Confusion Matrix - {MODEL_NAME.upper()} (Scenario 2)', fontsize=14, fontweight='bold')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig(f'/content/{MODEL_NAME}_scenario2_confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ“ Confusion matrix saved!")

# ============================================================================
# CREATE COMPREHENSIVE METRICS VISUALIZATION
# ============================================================================

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Cross-validation results (already have this, enhance it)
ax1 = axes[0, 0]
folds = [f'F{i+1}' for i in range(N_FOLDS)]
bars1 = ax1.bar(folds, fold_results, color='skyblue', edgecolor='navy', linewidth=1.5)
ax1.axhline(y=np.mean(fold_results), color='red', linestyle='--', linewidth=2,
           label=f'Mean: {np.mean(fold_results):.2f}%')
ax1.set_ylabel('Validation Accuracy (%)', fontsize=11)
ax1.set_title(f'{N_FOLDS}-Fold CV Results', fontsize=12, fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3, axis='y')
for i, v in enumerate(fold_results):
    ax1.text(i, v + 0.5, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)

# 2. All accuracy metrics
ax2 = axes[0, 1]
metric_names = ['Accuracy', 'Precision', 'Recall', 'Macro F1', 'Weighted F1']
metric_values = [test_accuracy, test_precision_macro, test_recall_macro, test_f1_macro, test_f1_weighted]
bars2 = ax2.barh(metric_names, metric_values, color='#3498db')
ax2.set_xlabel('Score (%)', fontsize=11)
ax2.set_title(f'All Metrics (Test Set)', fontsize=12, fontweight='bold')
ax2.set_xlim([0, 100])
for i, bar in enumerate(bars2):
    width = bar.get_width()
    ax2.text(width, bar.get_y() + bar.get_height()/2.,
            f' {width:.1f}%', ha='left', va='center', fontweight='bold', fontsize=9)

# 3. CV vs Test accuracy
ax3 = axes[0, 2]
comparison = [np.mean(fold_results), test_accuracy]
labels_comp = ['CV Mean', 'Test']
bars3 = ax3.bar(labels_comp, comparison, color=['#2ecc71', '#e74c3c'])
ax3.set_ylabel('Accuracy (%)', fontsize=11)
ax3.set_title('CV vs Test Accuracy', fontsize=12, fontweight='bold')
ax3.set_ylim([0, 100])
for bar in bars3:
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)

# 4. Time metrics
ax4 = axes[1, 0]
times = [overall_time * 60, avg_inference_per_image / 1000]  # Convert to seconds
labels_time = [f'Training\n({overall_time:.1f} min)', f'Inference\n({avg_inference_per_image:.2f} ms)']
colors = ['#f39c12', '#9b59b6']
bars4 = ax4.bar(labels_time, times, color=colors)
ax4.set_ylabel('Time (seconds)', fontsize=11)
ax4.set_title('Time Metrics', fontsize=12, fontweight='bold')
ax4.set_yscale('log')
for bar in bars4:
    height = bar.get_height()
    if height > 60:
        text = f'{height/60:.1f} min'
    else:
        text = f'{height:.2f} s'
    ax4.text(bar.get_x() + bar.get_width()/2., height,
            text, ha='center', va='bottom', fontweight='bold', fontsize=9)

# 5. F1 comparison
ax5 = axes[1, 1]
f1_types = ['Macro F1', 'Weighted F1']
f1_values = [test_f1_macro, test_f1_weighted]
bars5 = ax5.bar(f1_types, f1_values, color=['#1abc9c', '#16a085'])
ax5.set_ylabel('F1 Score (%)', fontsize=11)
ax5.set_title('F1 Score Comparison', fontsize=12, fontweight='bold')
ax5.set_ylim([0, 100])
for bar in bars5:
    height = bar.get_height()
    ax5.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)

# 6. Summary table
ax6 = axes[1, 2]
ax6.axis('off')
summary_data = [
    ['CV Mean', f"{np.mean(fold_results):.2f}% Â± {np.std(fold_results):.2f}%"],
    ['Test Acc', f"{test_accuracy:.2f}%"],
    ['Macro F1', f"{test_f1_macro:.2f}%"],
    ['Weighted F1', f"{test_f1_weighted:.2f}%"],
    ['Training', f"{overall_time:.2f} min"],
    ['Inference', f"{avg_inference_per_image:.2f} ms/img"],
    ['Parameters', f"{metrics['num_parameters']:,}"]
]
table = ax6.table(cellText=summary_data, cellLoc='left', loc='center',
                 colWidths=[0.5, 0.5], bbox=[0, 0, 1, 1])
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1, 2)
for i in range(len(summary_data)):
    table[(i, 0)].set_facecolor('#ecf0f1')
    table[(i, 0)].set_text_props(weight='bold')
ax6.set_title('Summary Metrics', fontsize=12, fontweight='bold', pad=20)

plt.tight_layout()
plt.savefig(f'/content/{MODEL_NAME}_scenario2_comprehensive_metrics.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ“ Comprehensive metrics visualization saved!")

# Classification Report
print("\nClassification Report:")
print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))

# Plot CV results
fig, ax = plt.subplots(figsize=(10, 6))

folds = [f'Fold {i+1}' for i in range(N_FOLDS)]
ax.bar(folds, fold_results, color='skyblue', edgecolor='navy', linewidth=1.5)
ax.axhline(y=np.mean(fold_results), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(fold_results):.2f}%')
ax.set_ylabel('Validation Accuracy (%)', fontsize=12)
ax.set_title(f'{N_FOLDS}-Fold Cross-Validation Results - {MODEL_NAME.upper()}', fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3, axis='y')

# Add value labels on bars
for i, v in enumerate(fold_results):
    ax.text(i, v + 1, f'{v:.2f}%', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.savefig(f'/content/{MODEL_NAME}_scenario2_cv_results.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ“ CV results visualization saved!")

# Copy all results to Google Drive
!mkdir -p "/content/drive/MyDrive/scenario2_results"

# Copy best fold model
!cp /content/{MODEL_NAME}_fold{best_fold_idx+1}_best.pth "/content/drive/MyDrive/scenario2_results/"

# Copy visualizations
!cp /content/{MODEL_NAME}_scenario2_confusion_matrix.png "/content/drive/MyDrive/scenario2_results/"
!cp /content/{MODEL_NAME}_scenario2_cv_results.png "/content/drive/MyDrive/scenario2_results/"
!cp /content/{MODEL_NAME}_scenario2_comprehensive_metrics.png "/content/drive/MyDrive/scenario2_results/"

# Save metrics
!cp /content/{MODEL_NAME}_scenario2_metrics.json "/content/drive/MyDrive/scenario2_results/"

# Save CV summary
with open(f'/content/{MODEL_NAME}_scenario2_cv_summary.txt', 'w') as f:
    f.write(f"Scenario 2 Cross-Validation Results - {MODEL_NAME.upper()}\n")
    f.write("="*70 + "\n\n")
    f.write("CROSS-VALIDATION:\n")
    for i, acc in enumerate(fold_results):
        f.write(f"  Fold {i+1}: {acc:.2f}%\n")
    f.write(f"\n  Mean: {np.mean(fold_results):.2f}% Â± {np.std(fold_results):.2f}%\n")
    f.write(f"  Best: Fold {best_fold_idx+1} ({best_fold_acc:.2f}%)\n\n")
    f.write("TEST SET METRICS:\n")
    f.write(f"  Accuracy:        {test_accuracy:.2f}%\n")
    f.write(f"  Precision:       {test_precision_macro:.2f}%\n")
    f.write(f"  Recall:          {test_recall_macro:.2f}%\n")
    f.write(f"  Macro F1:        {test_f1_macro:.2f}%\n")
    f.write(f"  Weighted F1:     {test_f1_weighted:.2f}%\n\n")
    f.write("TIME:\n")
    f.write(f"  Training:        {overall_time:.2f} min\n")
    f.write(f"  Inference:       {avg_inference_per_image:.2f} ms/image\n")

!cp /content/{MODEL_NAME}_scenario2_cv_summary.txt "/content/drive/MyDrive/scenario2_results/"

print("âœ“ All results saved to Google Drive!")
print("  Location: /MyDrive/scenario2_results/")
print("\nðŸ“ Files saved:")
print(f"  - {MODEL_NAME}_fold{best_fold_idx+1}_best.pth")
print(f"  - {MODEL_NAME}_scenario2_confusion_matrix.png")
print(f"  - {MODEL_NAME}_scenario2_cv_results.png")
print(f"  - {MODEL_NAME}_scenario2_comprehensive_metrics.png")
print(f"  - {MODEL_NAME}_scenario2_metrics.json")
print(f"  - {MODEL_NAME}_scenario2_cv_summary.txt")

"""Vit

"""

# ============================================================================
# CONFIGURATION - CHANGE THIS TO TRAIN DIFFERENT MODELS
# ============================================================================

MODEL_NAME = 'vit'  # 'attention', 'xgboost', or 'vit'
NUM_EPOCHS = 200  # More epochs for small dataset
LEARNING_RATE = 0.001  # Lower LR for small dataset
PATIENCE = 30
N_FOLDS = 5  # 5-fold cross-validation
BATCH_SIZE = 32  # Smaller batch for small dataset

print(f"Training {MODEL_NAME.upper()} with {N_FOLDS}-Fold Cross-Validation")
print(f"Epochs: {NUM_EPOCHS}, LR: {LEARNING_RATE}, Patience: {PATIENCE}")

# Setup K-Fold
kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)

# Track results across folds
fold_results = []
all_fold_histories = []

criterion = nn.CrossEntropyLoss()

print(f"\n{'='*70}")
print(f"Starting {N_FOLDS}-Fold Cross-Validation")
print(f"{'='*70}\n")

overall_start_time = time.time()

# K-Fold Cross-Validation Loop
for fold, (train_idx, val_idx) in enumerate(kfold.split(range(len(train_dataset)))):
    print(f"\n{'='*70}")
    print(f"FOLD {fold + 1}/{N_FOLDS}")
    print(f"{'='*70}")

    # Create data samplers
    train_sampler = SubsetRandomSampler(train_idx)
    val_sampler = SubsetRandomSampler(val_idx)

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        sampler=train_sampler,
        num_workers=2,
        pin_memory=True
    )

    val_loader = DataLoader(
        train_dataset,  # Use same dataset but different indices
        batch_size=BATCH_SIZE,
        sampler=val_sampler,
        num_workers=2,
        pin_memory=True
    )

    print(f"Train samples: {len(train_idx)}, Val samples: {len(val_idx)}")

    # Create fresh model for each fold
    if MODEL_NAME == 'attention':
        model = VGG13_Attention(num_classes=num_classes, dropout=0.6).to(device)
        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=15)
    elif MODEL_NAME == 'xgboost':
        model = VGG13_FeatureExtractor(num_classes=num_classes, dropout=0.6).to(device)
        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=15)
    elif MODEL_NAME == 'vit':
        model = VGG13_ViT(num_classes=num_classes, dropout=0.4).to(device)
        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.05)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)

    # Training loop for this fold
    best_val_acc = 0.0
    patience_counter = 0
    fold_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    for epoch in range(1, NUM_EPOCHS + 1):
        # Train
        train_loss, train_acc = train_one_epoch_cv(model, train_loader, criterion, optimizer, device)

        # Validate
        val_loss, val_acc = validate_cv(model, val_loader, criterion, device)

        # Update scheduler
        if MODEL_NAME in ['attention', 'xgboost']:
            scheduler.step(val_acc)
        else:
            scheduler.step()

        # Save history
        fold_history['train_loss'].append(train_loss)
        fold_history['train_acc'].append(train_acc)
        fold_history['val_loss'].append(val_loss)
        fold_history['val_acc'].append(val_acc)

        print(f"Epoch {epoch}/{NUM_EPOCHS}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%")

        # Save best model for this fold
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0

            # Save checkpoint
            torch.save({
                'model': model.state_dict(),
                'fold': fold,
                'best_val_acc': best_val_acc
            }, f'/content/{MODEL_NAME}_fold{fold+1}_best.pth')
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                print(f"\nEarly stopping at epoch {epoch}")
                break

    print(f"\nFold {fold + 1} Best Val Accuracy: {best_val_acc:.2f}%")
    fold_results.append(best_val_acc)
    all_fold_histories.append(fold_history)

overall_time = (time.time() - overall_start_time) / 60

# Print cross-validation summary
print(f"\n{'='*70}")
print(f"CROSS-VALIDATION SUMMARY")
print(f"{'='*70}")
for i, acc in enumerate(fold_results):
    print(f"Fold {i+1}: {acc:.2f}%")
print(f"{'-'*70}")
print(f"Mean CV Accuracy: {np.mean(fold_results):.2f}% Â± {np.std(fold_results):.2f}%")
print(f"Total Training Time: {overall_time:.2f} minutes")
print(f"{'='*70}")

# ============================================================================
# COMPREHENSIVE EVALUATION WITH ALL METRICS
# ============================================================================

# Find best fold
best_fold_idx = np.argmax(fold_results)
best_fold_acc = fold_results[best_fold_idx]

print(f"Best fold: Fold {best_fold_idx + 1} with {best_fold_acc:.2f}% validation accuracy")

# Load best fold model
if MODEL_NAME == 'attention':
    final_model = VGG13_Attention(num_classes=num_classes, dropout=0.6).to(device)
elif MODEL_NAME == 'xgboost':
    final_model = VGG13_FeatureExtractor(num_classes=num_classes, dropout=0.6).to(device)
elif MODEL_NAME == 'vit':
    final_model = VGG13_ViT(num_classes=num_classes, dropout=0.4).to(device)

checkpoint = torch.load(f'/content/{MODEL_NAME}_fold{best_fold_idx+1}_best.pth')
final_model.load_state_dict(checkpoint['model'])
final_model.eval()

print(f"âœ“ Loaded best fold model")

# Create test loader
test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True
)

# Test with all metrics
print("\nTesting on held-out test set...")

inference_times = []
all_preds = []
all_labels = []
correct = 0
total = 0

with torch.no_grad():
    for images, labels in tqdm(test_loader, desc='Testing'):
        images, labels = images.to(device), labels.to(device)

        # Measure inference time
        start_time = time.time()
        outputs = final_model(images)
        inference_time = (time.time() - start_time) * 1000  # ms
        inference_times.append(inference_time)

        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate all metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

test_accuracy = 100. * correct / total
test_precision_macro = precision_score(all_labels, all_preds, average='macro') * 100
test_recall_macro = recall_score(all_labels, all_preds, average='macro') * 100
test_f1_macro = f1_score(all_labels, all_preds, average='macro') * 100
test_f1_weighted = f1_score(all_labels, all_preds, average='weighted') * 100

# Inference time metrics
avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
avg_inference_per_image = avg_inference_time / BATCH_SIZE

# Print comprehensive results
print(f"\n{'='*70}")
print(f"SCENARIO 2 COMPREHENSIVE RESULTS - {MODEL_NAME.upper()}")
print(f"{'='*70}")
print(f"\nðŸ“Š CROSS-VALIDATION RESULTS:")
print(f"  Fold 1:  {fold_results[0]:.2f}%")
print(f"  Fold 2:  {fold_results[1]:.2f}%")
print(f"  Fold 3:  {fold_results[2]:.2f}%")
print(f"  Fold 4:  {fold_results[3]:.2f}%")
print(f"  Fold 5:  {fold_results[4]:.2f}%")
print(f"  {'â”€'*50}")
print(f"  Mean CV Accuracy:  {np.mean(fold_results):.2f}% Â± {np.std(fold_results):.2f}%")
print(f"  Best Fold:         Fold {best_fold_idx + 1} ({best_fold_acc:.2f}%)")
print(f"\nðŸ“Š TEST SET METRICS:")
print(f"  Test Accuracy:      {test_accuracy:.2f}%")
print(f"\nðŸ“ˆ PRECISION & RECALL:")
print(f"  Precision (Macro):  {test_precision_macro:.2f}%")
print(f"  Recall (Macro):     {test_recall_macro:.2f}%")
print(f"\nðŸŽ¯ F1 SCORES:")
print(f"  Macro F1:           {test_f1_macro:.2f}%")
print(f"  Weighted F1:        {test_f1_weighted:.2f}%")
print(f"\nâ±ï¸ TIME METRICS:")
print(f"  Training Time:      {overall_time:.2f} minutes ({N_FOLDS} folds)")
print(f"  Avg Inference Time: {avg_inference_time:.2f} ms/batch ({avg_inference_per_image:.2f} ms/image)")
print(f"  Std Inference Time: {std_inference_time:.2f} ms/batch")
print(f"{'='*70}")

# Save metrics
metrics = {
    'model_name': MODEL_NAME,
    'scenario': 2,
    # Cross-validation
    'cv_fold_1': fold_results[0],
    'cv_fold_2': fold_results[1],
    'cv_fold_3': fold_results[2],
    'cv_fold_4': fold_results[3],
    'cv_fold_5': fold_results[4],
    'cv_mean': np.mean(fold_results),
    'cv_std': np.std(fold_results),
    'best_fold': best_fold_idx + 1,
    'best_fold_acc': best_fold_acc,
    # Test set metrics
    'test_accuracy': test_accuracy,
    'precision_macro': test_precision_macro,
    'recall_macro': test_recall_macro,
    'f1_macro': test_f1_macro,
    'f1_weighted': test_f1_weighted,
    # Time metrics
    'training_time_minutes': overall_time,
    'training_time_per_fold': overall_time / N_FOLDS,
    'inference_time_ms_per_batch': avg_inference_time,
    'inference_time_ms_per_image': avg_inference_per_image,
    'inference_time_std': std_inference_time,
    # Additional info
    'num_parameters': sum(p.numel() for p in final_model.parameters()),
    'batch_size': BATCH_SIZE,
    'n_folds': N_FOLDS
}

# Save as JSON
import json
with open(f'/content/{MODEL_NAME}_scenario2_metrics.json', 'w') as f:
    json.dump(metrics, f, indent=4)

print(f"\nâœ“ Metrics saved to {MODEL_NAME}_scenario2_metrics.json")

# Confusion Matrix
cm = confusion_matrix(all_labels, all_preds)

plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title(f'Confusion Matrix - {MODEL_NAME.upper()} (Scenario 2)', fontsize=14, fontweight='bold')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig(f'/content/{MODEL_NAME}_scenario2_confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ“ Confusion matrix saved!")

# ============================================================================
# CREATE COMPREHENSIVE METRICS VISUALIZATION
# ============================================================================

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Cross-validation results (already have this, enhance it)
ax1 = axes[0, 0]
folds = [f'F{i+1}' for i in range(N_FOLDS)]
bars1 = ax1.bar(folds, fold_results, color='skyblue', edgecolor='navy', linewidth=1.5)
ax1.axhline(y=np.mean(fold_results), color='red', linestyle='--', linewidth=2,
           label=f'Mean: {np.mean(fold_results):.2f}%')
ax1.set_ylabel('Validation Accuracy (%)', fontsize=11)
ax1.set_title(f'{N_FOLDS}-Fold CV Results', fontsize=12, fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3, axis='y')
for i, v in enumerate(fold_results):
    ax1.text(i, v + 0.5, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)

# 2. All accuracy metrics
ax2 = axes[0, 1]
metric_names = ['Accuracy', 'Precision', 'Recall', 'Macro F1', 'Weighted F1']
metric_values = [test_accuracy, test_precision_macro, test_recall_macro, test_f1_macro, test_f1_weighted]
bars2 = ax2.barh(metric_names, metric_values, color='#3498db')
ax2.set_xlabel('Score (%)', fontsize=11)
ax2.set_title(f'All Metrics (Test Set)', fontsize=12, fontweight='bold')
ax2.set_xlim([0, 100])
for i, bar in enumerate(bars2):
    width = bar.get_width()
    ax2.text(width, bar.get_y() + bar.get_height()/2.,
            f' {width:.1f}%', ha='left', va='center', fontweight='bold', fontsize=9)

# 3. CV vs Test accuracy
ax3 = axes[0, 2]
comparison = [np.mean(fold_results), test_accuracy]
labels_comp = ['CV Mean', 'Test']
bars3 = ax3.bar(labels_comp, comparison, color=['#2ecc71', '#e74c3c'])
ax3.set_ylabel('Accuracy (%)', fontsize=11)
ax3.set_title('CV vs Test Accuracy', fontsize=12, fontweight='bold')
ax3.set_ylim([0, 100])
for bar in bars3:
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)

# 4. Time metrics
ax4 = axes[1, 0]
times = [overall_time * 60, avg_inference_per_image / 1000]  # Convert to seconds
labels_time = [f'Training\n({overall_time:.1f} min)', f'Inference\n({avg_inference_per_image:.2f} ms)']
colors = ['#f39c12', '#9b59b6']
bars4 = ax4.bar(labels_time, times, color=colors)
ax4.set_ylabel('Time (seconds)', fontsize=11)
ax4.set_title('Time Metrics', fontsize=12, fontweight='bold')
ax4.set_yscale('log')
for bar in bars4:
    height = bar.get_height()
    if height > 60:
        text = f'{height/60:.1f} min'
    else:
        text = f'{height:.2f} s'
    ax4.text(bar.get_x() + bar.get_width()/2., height,
            text, ha='center', va='bottom', fontweight='bold', fontsize=9)

# 5. F1 comparison
ax5 = axes[1, 1]
f1_types = ['Macro F1', 'Weighted F1']
f1_values = [test_f1_macro, test_f1_weighted]
bars5 = ax5.bar(f1_types, f1_values, color=['#1abc9c', '#16a085'])
ax5.set_ylabel('F1 Score (%)', fontsize=11)
ax5.set_title('F1 Score Comparison', fontsize=12, fontweight='bold')
ax5.set_ylim([0, 100])
for bar in bars5:
    height = bar.get_height()
    ax5.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)

# 6. Summary table
ax6 = axes[1, 2]
ax6.axis('off')
summary_data = [
    ['CV Mean', f"{np.mean(fold_results):.2f}% Â± {np.std(fold_results):.2f}%"],
    ['Test Acc', f"{test_accuracy:.2f}%"],
    ['Macro F1', f"{test_f1_macro:.2f}%"],
    ['Weighted F1', f"{test_f1_weighted:.2f}%"],
    ['Training', f"{overall_time:.2f} min"],
    ['Inference', f"{avg_inference_per_image:.2f} ms/img"],
    ['Parameters', f"{metrics['num_parameters']:,}"]
]
table = ax6.table(cellText=summary_data, cellLoc='left', loc='center',
                 colWidths=[0.5, 0.5], bbox=[0, 0, 1, 1])
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1, 2)
for i in range(len(summary_data)):
    table[(i, 0)].set_facecolor('#ecf0f1')
    table[(i, 0)].set_text_props(weight='bold')
ax6.set_title('Summary Metrics', fontsize=12, fontweight='bold', pad=20)

plt.tight_layout()
plt.savefig(f'/content/{MODEL_NAME}_scenario2_comprehensive_metrics.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ“ Comprehensive metrics visualization saved!")

# Classification Report
print("\nClassification Report:")
print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))

# Plot CV results
fig, ax = plt.subplots(figsize=(10, 6))

folds = [f'Fold {i+1}' for i in range(N_FOLDS)]
ax.bar(folds, fold_results, color='skyblue', edgecolor='navy', linewidth=1.5)
ax.axhline(y=np.mean(fold_results), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(fold_results):.2f}%')
ax.set_ylabel('Validation Accuracy (%)', fontsize=12)
ax.set_title(f'{N_FOLDS}-Fold Cross-Validation Results - {MODEL_NAME.upper()}', fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3, axis='y')

# Add value labels on bars
for i, v in enumerate(fold_results):
    ax.text(i, v + 1, f'{v:.2f}%', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.savefig(f'/content/{MODEL_NAME}_scenario2_cv_results.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ“ CV results visualization saved!")

# Copy all results to Google Drive
!mkdir -p "/content/drive/MyDrive/scenario2_results"

# Copy best fold model
!cp /content/{MODEL_NAME}_fold{best_fold_idx+1}_best.pth "/content/drive/MyDrive/scenario2_results/"

# Copy visualizations
!cp /content/{MODEL_NAME}_scenario2_confusion_matrix.png "/content/drive/MyDrive/scenario2_results/"
!cp /content/{MODEL_NAME}_scenario2_cv_results.png "/content/drive/MyDrive/scenario2_results/"
!cp /content/{MODEL_NAME}_scenario2_comprehensive_metrics.png "/content/drive/MyDrive/scenario2_results/"

# Save metrics
!cp /content/{MODEL_NAME}_scenario2_metrics.json "/content/drive/MyDrive/scenario2_results/"

# Save CV summary
with open(f'/content/{MODEL_NAME}_scenario2_cv_summary.txt', 'w') as f:
    f.write(f"Scenario 2 Cross-Validation Results - {MODEL_NAME.upper()}\n")
    f.write("="*70 + "\n\n")
    f.write("CROSS-VALIDATION:\n")
    for i, acc in enumerate(fold_results):
        f.write(f"  Fold {i+1}: {acc:.2f}%\n")
    f.write(f"\n  Mean: {np.mean(fold_results):.2f}% Â± {np.std(fold_results):.2f}%\n")
    f.write(f"  Best: Fold {best_fold_idx+1} ({best_fold_acc:.2f}%)\n\n")
    f.write("TEST SET METRICS:\n")
    f.write(f"  Accuracy:        {test_accuracy:.2f}%\n")
    f.write(f"  Precision:       {test_precision_macro:.2f}%\n")
    f.write(f"  Recall:          {test_recall_macro:.2f}%\n")
    f.write(f"  Macro F1:        {test_f1_macro:.2f}%\n")
    f.write(f"  Weighted F1:     {test_f1_weighted:.2f}%\n\n")
    f.write("TIME:\n")
    f.write(f"  Training:        {overall_time:.2f} min\n")
    f.write(f"  Inference:       {avg_inference_per_image:.2f} ms/image\n")

!cp /content/{MODEL_NAME}_scenario2_cv_summary.txt "/content/drive/MyDrive/scenario2_results/"

print("âœ“ All results saved to Google Drive!")
print("  Location: /MyDrive/scenario2_results/")
print("\nðŸ“ Files saved:")
print(f"  - {MODEL_NAME}_fold{best_fold_idx+1}_best.pth")
print(f"  - {MODEL_NAME}_scenario2_confusion_matrix.png")
print(f"  - {MODEL_NAME}_scenario2_cv_results.png")
print(f"  - {MODEL_NAME}_scenario2_comprehensive_metrics.png")
print(f"  - {MODEL_NAME}_scenario2_metrics.json")
print(f"  - {MODEL_NAME}_scenario2_cv_summary.txt")