{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "import seaborn as sns\n",
        "import time\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# VGG13 Architecture (Training from Scratch)\n",
        "# ============================================================================\n",
        "class VGG13(nn.Module):\n",
        "    def __init__(self, num_classes, dropout=0.5):\n",
        "        super(VGG13, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 4\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 5\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ResNet18 Architecture (Training from Scratch)\n",
        "# ============================================================================\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10, dropout=0.5):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Early Stopping\n",
        "# ============================================================================\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0.001, mode='max'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, score):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'max':\n",
        "            if score > self.best_score + self.min_delta:\n",
        "                self.best_score = score\n",
        "                self.counter = 0\n",
        "            else:\n",
        "                self.counter += 1\n",
        "        else:  # min\n",
        "            if score < self.best_score - self.min_delta:\n",
        "                self.best_score = score\n",
        "                self.counter = 0\n",
        "            else:\n",
        "                self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            self.early_stop = True\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Custom Dataset with Albumentations\n",
        "# ============================================================================\n",
        "class PlantDiseaseDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        image = np.array(image)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# OPTIMIZED Augmentation Strategies\n",
        "# ============================================================================\n",
        "def get_basic_augmentation(img_size=224):\n",
        "    \"\"\"Basic augmentation - fastest\"\"\"\n",
        "    train_transform = A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Rotate(limit=10, p=0.5),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    val_transform = A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "\n",
        "def get_moderate_augmentation(img_size=224):\n",
        "    \"\"\"Moderate augmentation - balanced speed/performance\"\"\"\n",
        "    train_transform = A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.2),\n",
        "        A.Rotate(limit=20, p=0.5),\n",
        "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=15, p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "        A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=20, val_shift_limit=15, p=0.3),\n",
        "        A.OneOf([\n",
        "            A.GaussianBlur(blur_limit=(3, 5), p=1),\n",
        "            A.GaussNoise(var_limit=(10.0, 30.0), p=1),\n",
        "        ], p=0.3),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    val_transform = A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "\n",
        "def get_aggressive_augmentation(img_size=224):\n",
        "    \"\"\"Aggressive augmentation - use only if needed\"\"\"\n",
        "    train_transform = A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.3),\n",
        "        A.Rotate(limit=25, p=0.5),\n",
        "        A.ShiftScaleRotate(shift_limit=0.15, scale_limit=0.2, rotate_limit=20, p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.6),\n",
        "        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=25, val_shift_limit=20, p=0.5),\n",
        "        A.OneOf([\n",
        "            A.GaussianBlur(blur_limit=(3, 5), p=1),\n",
        "            A.GaussNoise(var_limit=(10.0, 40.0), p=1),\n",
        "        ], p=0.4),\n",
        "        A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.3),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    val_transform = A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Data Loading Functions\n",
        "# ============================================================================\n",
        "def load_dataset_type1(root_dir, split='train'):\n",
        "    \"\"\"Load Exp1: dataset/class1/train/, dataset/class1/validation/, dataset/class1/test/\"\"\"\n",
        "    root_path = Path(root_dir)\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    class_names = []\n",
        "\n",
        "    class_dirs = sorted([d for d in root_path.iterdir() if d.is_dir()])\n",
        "\n",
        "    for class_idx, class_dir in enumerate(class_dirs):\n",
        "        class_name = class_dir.name\n",
        "        class_names.append(class_name)\n",
        "\n",
        "        split_dir = class_dir / split\n",
        "        if not split_dir.exists():\n",
        "            print(f\"Warning: {split_dir} does not exist, skipping...\")\n",
        "            continue\n",
        "\n",
        "        for ext in ['*.jpg', '*.JPG', '*.png', '*.PNG', '*.jpeg', '*.JPEG']:\n",
        "            for img_path in split_dir.glob(ext):\n",
        "                image_paths.append(str(img_path))\n",
        "                labels.append(class_idx)\n",
        "\n",
        "    return image_paths, labels, class_names\n",
        "\n",
        "\n",
        "def load_dataset_type2(root_dir, split='train', class_mapping=None):\n",
        "    \"\"\"Load Exp2: dataset/train/class1/, dataset/test/class1/\"\"\"\n",
        "    root_path = Path(root_dir) / split\n",
        "\n",
        "    if not root_path.exists():\n",
        "        raise ValueError(f\"Directory does not exist: {root_path}\")\n",
        "\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    found_class_names = []\n",
        "\n",
        "    class_dirs = sorted([d for d in root_path.iterdir() if d.is_dir()])\n",
        "\n",
        "    for class_dir in class_dirs:\n",
        "        class_name = class_dir.name\n",
        "\n",
        "        if class_mapping is not None:\n",
        "            if class_name in class_mapping:\n",
        "                class_idx = class_mapping[class_name]\n",
        "            else:\n",
        "                print(f\"Warning: Class '{class_name}' not in mapping, skipping...\")\n",
        "                continue\n",
        "        else:\n",
        "            class_idx = len(found_class_names)\n",
        "            found_class_names.append(class_name)\n",
        "\n",
        "        for ext in ['*.jpg', '*.JPG', '*.png', '*.PNG', '*.jpeg', '*.JPEG']:\n",
        "            for img_path in class_dir.glob(ext):\n",
        "                image_paths.append(str(img_path))\n",
        "                labels.append(class_idx)\n",
        "\n",
        "    if class_mapping is None:\n",
        "        return image_paths, labels, found_class_names\n",
        "    else:\n",
        "        return image_paths, labels, None\n",
        "\n",
        "\n",
        "def analyze_dataset(image_paths, labels, class_names, name=\"Dataset\"):\n",
        "    \"\"\"Analyze dataset distribution\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{name} Analysis\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total images: {len(image_paths)}\")\n",
        "    print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "    class_counts = Counter(labels)\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    for idx, name in enumerate(class_names):\n",
        "        count = class_counts.get(idx, 0)\n",
        "        percentage = (count / len(labels) * 100) if len(labels) > 0 else 0\n",
        "        print(f\"  {name}: {count} images ({percentage:.1f}%)\")\n",
        "\n",
        "    if len(class_counts) > 0:\n",
        "        max_count = max(class_counts.values())\n",
        "        min_count = min(class_counts.values())\n",
        "        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
        "        print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}\")\n",
        "        if imbalance_ratio > 3:\n",
        "            print(\"⚠️  Severe class imbalance detected! Will use class weights in loss.\")\n",
        "\n",
        "    return class_counts\n",
        "\n",
        "\n",
        "def create_dataloader(image_paths, labels, transform, batch_size=32,\n",
        "                      shuffle=True, num_workers=4):\n",
        "    \"\"\"\n",
        "    OPTIMIZED: Create dataloader WITHOUT weighted sampling\n",
        "    Use class weights in loss function instead for better performance\n",
        "    \"\"\"\n",
        "\n",
        "    if not image_paths:\n",
        "        raise ValueError(\"No images found!\")\n",
        "\n",
        "    # Calculate class weights using Inverse Frequency method\n",
        "    class_counts = np.bincount(labels)\n",
        "    num_classes = len(class_counts)\n",
        "    total_samples = len(labels)\n",
        "\n",
        "    # Inverse Frequency Weights: Weight = Total Samples / Class Count\n",
        "    class_weights = total_samples / class_counts\n",
        "    class_weights = class_weights / class_weights.sum() * num_classes\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"CLASS WEIGHTS FOR LOSS FUNCTION\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total Samples: {total_samples}\")\n",
        "    print(f\"Number of Classes: {num_classes}\")\n",
        "    print(f\"Calculated Class Weights (Inverse Frequency):\")\n",
        "    for i, weight in enumerate(class_weights):\n",
        "        print(f\"  Class {i}: Count={class_counts[i]}, Weight={weight:.4f}\")\n",
        "    print(\"✓ Class weights will be used in CrossEntropyLoss\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = PlantDiseaseDataset(image_paths, labels, transform)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True,  # Keep workers alive between epochs\n",
        "        prefetch_factor=2  # Prefetch batches\n",
        "    )\n",
        "\n",
        "    return loader, class_weights\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Training and Evaluation Functions\n",
        "# ============================================================================\n",
        "def train_epoch(model, loader, criterion, optimizer, scaler, device, clip_grad=1.0):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    pbar = tqdm(loader, desc='Training')\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)  # Faster than zero_grad()\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        pbar.set_postfix({'loss': running_loss/len(loader), 'acc': 100.*correct/total})\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    return running_loss / len(loader), 100. * correct / total, epoch_time\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device, phase='val'):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    inference_times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=f'{phase.capitalize()}'):\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            start_time = time.time()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            end_time = time.time()\n",
        "\n",
        "            batch_time = (end_time - start_time) / inputs.size(0) * 1000\n",
        "            inference_times.append(batch_time)\n",
        "\n",
        "            if criterion is not None:\n",
        "                loss = criterion(outputs, labels.to(device))\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "    macro_f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "    weighted_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "\n",
        "    avg_loss = running_loss / len(loader) if criterion is not None else 0.0\n",
        "\n",
        "    metrics = {\n",
        "        'loss': avg_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'macro_f1': macro_f1,\n",
        "        'weighted_f1': weighted_f1,\n",
        "        'avg_inference_time_ms': avg_inference_time,\n",
        "        'predictions': all_predictions,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Plotting Functions\n",
        "# ============================================================================\n",
        "def plot_training_history(history, save_name='training_history.png'):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    fig.suptitle('Training History', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Loss\n",
        "    axes[0, 0].plot(history['train_loss'], label='Train', linewidth=2, marker='o', markersize=4)\n",
        "    axes[0, 0].plot(history['val_loss'], label='Val', linewidth=2, marker='s', markersize=4)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy\n",
        "    axes[0, 1].plot(history['train_acc'], label='Train', linewidth=2, marker='o', markersize=4)\n",
        "    axes[0, 1].plot(history['val_acc'], label='Val', linewidth=2, marker='s', markersize=4)\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].set_title('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Precision & Recall\n",
        "    axes[0, 2].plot(history['val_precision'], label='Precision', linewidth=2, marker='o', markersize=4)\n",
        "    axes[0, 2].plot(history['val_recall'], label='Recall', linewidth=2, marker='s', markersize=4)\n",
        "    axes[0, 2].set_xlabel('Epoch')\n",
        "    axes[0, 2].set_ylabel('Score')\n",
        "    axes[0, 2].set_title('Precision & Recall')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # F1 Scores\n",
        "    axes[1, 0].plot(history['val_macro_f1'], label='Macro F1', linewidth=2, marker='o', markersize=4)\n",
        "    axes[1, 0].plot(history['val_weighted_f1'], label='Weighted F1', linewidth=2, marker='s', markersize=4)\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('F1 Score')\n",
        "    axes[1, 0].set_title('F1 Scores')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Training Time\n",
        "    axes[1, 1].plot(history['train_time'], linewidth=2, color='orange', marker='o', markersize=4)\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Time (seconds)')\n",
        "    axes[1, 1].set_title('Training Time per Epoch')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Summary Table\n",
        "    axes[1, 2].axis('off')\n",
        "    summary_data = [\n",
        "        ['Metric', 'Value'],\n",
        "        ['Best Val Acc', f\"{max(history['val_acc']):.4f}\"],\n",
        "        ['Final Val Acc', f\"{history['val_acc'][-1]:.4f}\"],\n",
        "        ['Final Precision', f\"{history['val_precision'][-1]:.4f}\"],\n",
        "        ['Final Recall', f\"{history['val_recall'][-1]:.4f}\"],\n",
        "        ['Final Macro F1', f\"{history['val_macro_f1'][-1]:.4f}\"],\n",
        "        ['Final Weighted F1', f\"{history['val_weighted_f1'][-1]:.4f}\"],\n",
        "        ['Avg Epoch Time', f\"{np.mean(history['train_time']):.2f}s\"],\n",
        "    ]\n",
        "    table = axes[1, 2].table(cellText=summary_data, cellLoc='left', loc='center', colWidths=[0.6, 0.3])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(9)\n",
        "    table.scale(1, 2)\n",
        "    for i in range(2):\n",
        "        table[(0, i)].set_facecolor('#4CAF50')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "    axes[1, 2].set_title('Summary', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_name, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved: {save_name}\")\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names, save_name='confusion_matrix.png'):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(max(12, len(class_names)//2), max(10, len(class_names)//2)))\n",
        "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "    plt.yticks(rotation=0, fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_name, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved: {save_name}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# OPTIMIZED Universal Training Function\n",
        "# ============================================================================\n",
        "def train_model(model, model_name, dataset_dir, dataset_type, num_epochs, batch_size,\n",
        "                lr, img_size, experiment_name, augmentation_level='moderate', num_workers=4):\n",
        "    \"\"\"\n",
        "    OPTIMIZED universal training function\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{experiment_name.upper()}: {model_name.upper()} FROM SCRATCH\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Load data based on dataset type\n",
        "    if dataset_type == 1:\n",
        "        print(\"Dataset Structure: dataset/class/train | validation | test\")\n",
        "        print(f\"\\nLoading training data from: {dataset_dir}\")\n",
        "        train_imgs, train_lbls, class_names = load_dataset_type1(dataset_dir, split='train')\n",
        "        analyze_dataset(train_imgs, train_lbls, class_names, \"Training Set\")\n",
        "\n",
        "        print(f\"\\nLoading validation data from: {dataset_dir}\")\n",
        "        val_imgs, val_lbls, _ = load_dataset_type1(dataset_dir, split='validation')\n",
        "        analyze_dataset(val_imgs, val_lbls, class_names, \"Validation Set\")\n",
        "\n",
        "        print(f\"\\nLoading test data from: {dataset_dir}\")\n",
        "        test_imgs, test_lbls, _ = load_dataset_type1(dataset_dir, split='test')\n",
        "        analyze_dataset(test_imgs, test_lbls, class_names, \"Test Set\")\n",
        "\n",
        "    else:  # dataset_type == 2\n",
        "        print(\"Dataset Structure: dataset/train/class | test/class\")\n",
        "        print(f\"\\nLoading training data from: {dataset_dir}/train\")\n",
        "        train_imgs, train_lbls, class_names = load_dataset_type2(dataset_dir, split='train')\n",
        "        analyze_dataset(train_imgs, train_lbls, class_names, \"Training Set\")\n",
        "\n",
        "        print(\"\\n✓ Creating validation split (15% of training data)\")\n",
        "        train_imgs, val_imgs, train_lbls, val_lbls = train_test_split(\n",
        "            train_imgs, train_lbls, test_size=0.15, stratify=train_lbls, random_state=42\n",
        "        )\n",
        "        print(f\"  Training samples after split: {len(train_imgs)}\")\n",
        "        print(f\"  Validation samples: {len(val_imgs)}\")\n",
        "\n",
        "        print(f\"\\nLoading test data from: {dataset_dir}/test\")\n",
        "        class_mapping = {name: idx for idx, name in enumerate(class_names)}\n",
        "        test_imgs, test_lbls, _ = load_dataset_type2(dataset_dir, split='test', class_mapping=class_mapping)\n",
        "        analyze_dataset(test_imgs, test_lbls, class_names, \"Test Set\")\n",
        "\n",
        "    # Get augmentation based on level\n",
        "    if augmentation_level == 'aggressive':\n",
        "        train_transform, val_transform = get_aggressive_augmentation(img_size)\n",
        "        print(\"\\n✓ Using AGGRESSIVE augmentation\")\n",
        "    elif augmentation_level == 'moderate':\n",
        "        train_transform, val_transform = get_moderate_augmentation(img_size)\n",
        "        print(\"\\n✓ Using MODERATE augmentation (RECOMMENDED for best speed/performance)\")\n",
        "    else:\n",
        "        train_transform, val_transform = get_basic_augmentation(img_size)\n",
        "        print(\"\\n✓ Using BASIC augmentation\")\n",
        "\n",
        "    # Create dataloaders - NO weighted sampling, use class weights in loss instead\n",
        "    train_loader, class_weights = create_dataloader(\n",
        "        train_imgs, train_lbls, train_transform,\n",
        "        batch_size, shuffle=True, num_workers=num_workers\n",
        "    )\n",
        "    val_loader, _ = create_dataloader(\n",
        "        val_imgs, val_lbls, val_transform,\n",
        "        batch_size, shuffle=False, num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Training setup with class weights in loss function\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device), label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "    early_stopping = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [], 'train_time': [],\n",
        "        'val_loss': [], 'val_acc': [], 'val_precision': [],\n",
        "        'val_recall': [], 'val_macro_f1': [], 'val_weighted_f1': []\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"TRAINING\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total batches per epoch: {len(train_loader)}\")\n",
        "    print(f\"Estimated epoch time: ~{len(train_loader) * 0.5 / 60:.1f} minutes\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    overall_start = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        train_loss, train_acc, train_time = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device\n",
        "        )\n",
        "\n",
        "        val_metrics = evaluate(model, val_loader, criterion, device, 'val')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Store metrics\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['train_time'].append(train_time)\n",
        "        history['val_loss'].append(val_metrics['loss'])\n",
        "        history['val_acc'].append(val_metrics['accuracy'])\n",
        "        history['val_precision'].append(val_metrics['precision'])\n",
        "        history['val_recall'].append(val_metrics['recall'])\n",
        "        history['val_macro_f1'].append(val_metrics['macro_f1'])\n",
        "        history['val_weighted_f1'].append(val_metrics['weighted_f1'])\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {train_time/60:.2f} min\")\n",
        "        print(f\"Val Acc: {val_metrics['accuracy']:.4f}, Val F1: {val_metrics['weighted_f1']:.4f}\")\n",
        "\n",
        "        if val_metrics['accuracy'] > best_val_acc:\n",
        "            best_val_acc = val_metrics['accuracy']\n",
        "            torch.save(model.state_dict(), f'{experiment_name}_best_model.pth')\n",
        "            print(f\"✓ Saved best model! Acc: {best_val_acc:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopping(val_metrics['accuracy']):\n",
        "            print(f\"\\n⚠️  Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    total_training_time = time.time() - overall_start\n",
        "\n",
        "    # Load best model for testing\n",
        "    model.load_state_dict(torch.load(f'{experiment_name}_best_model.pth'))\n",
        "\n",
        "    # Test\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"TESTING\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    test_loader, _ = create_dataloader(\n",
        "        test_imgs, test_lbls, val_transform,\n",
        "        batch_size, shuffle=False, num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    test_metrics = evaluate(model, test_loader, None, device, 'test')\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{experiment_name.upper()} FINAL RESULTS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Model:                 {model_name.upper()}\")\n",
        "    print(f\"Accuracy:              {test_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision:             {test_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall:                {test_metrics['recall']:.4f}\")\n",
        "    print(f\"Macro F1:              {test_metrics['macro_f1']:.4f}\")\n",
        "    print(f\"Weighted F1:           {test_metrics['weighted_f1']:.4f}\")\n",
        "    print(f\"Training Time:         {total_training_time:.2f} seconds ({total_training_time/60:.2f} minutes)\")\n",
        "    print(f\"Inference Time:        {test_metrics['avg_inference_time_ms']:.2f} ms per image\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Save results\n",
        "    results = {\n",
        "        'model': model_name,\n",
        "        'accuracy': float(test_metrics['accuracy']),\n",
        "        'precision': float(test_metrics['precision']),\n",
        "        'recall': float(test_metrics['recall']),\n",
        "        'macro_f1': float(test_metrics['macro_f1']),\n",
        "        'weighted_f1': float(test_metrics['weighted_f1']),\n",
        "        'training_time_seconds': float(total_training_time),\n",
        "        'training_time_minutes': float(total_training_time / 60),\n",
        "        'inference_time_ms': float(test_metrics['avg_inference_time_ms'])\n",
        "    }\n",
        "\n",
        "    with open(f'{experiment_name}_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    print(f\"✓ Saved: {experiment_name}_results.json\")\n",
        "\n",
        "    # Plot\n",
        "    plot_training_history(history, f'{experiment_name}_training_history.png')\n",
        "    plot_confusion_matrix(test_metrics['labels'], test_metrics['predictions'],\n",
        "                         class_names, f'{experiment_name}_confusion_matrix.png')\n",
        "\n",
        "    print(\"\\nPer-Class Classification Report:\")\n",
        "    print(classification_report(\n",
        "        test_metrics['labels'],\n",
        "        test_metrics['predictions'],\n",
        "        target_names=class_names,\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    return results, history\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# OPTIMIZED Cross-Validation Training Function\n",
        "# ============================================================================\n",
        "def train_model_cv(model_class, model_name, dataset_dir, dataset_type, num_epochs, batch_size,\n",
        "                   lr, img_size, experiment_name, augmentation_level='moderate', n_splits=5,\n",
        "                   dropout=0.5, num_workers=4):\n",
        "    \"\"\"\n",
        "    OPTIMIZED K-Fold Cross-Validation training\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{experiment_name.upper()}: {model_name.upper()} WITH {n_splits}-FOLD CV\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Load ALL data (train + test combined for CV)\n",
        "    print(f\"Loading training data from: {dataset_dir}/train\")\n",
        "    train_imgs, train_lbls, class_names = load_dataset_type2(dataset_dir, split='train')\n",
        "\n",
        "    print(f\"Loading test data from: {dataset_dir}/test\")\n",
        "    class_mapping = {name: idx for idx, name in enumerate(class_names)}\n",
        "    test_imgs, test_lbls, _ = load_dataset_type2(dataset_dir, split='test', class_mapping=class_mapping)\n",
        "\n",
        "    # Combine train and test for cross-validation\n",
        "    all_imgs = train_imgs + test_imgs\n",
        "    all_lbls = train_lbls + test_lbls\n",
        "\n",
        "    print(f\"\\n✓ Total samples for CV: {len(all_imgs)}\")\n",
        "    analyze_dataset(all_imgs, all_lbls, class_names, \"Complete Dataset for CV\")\n",
        "\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    # Get augmentation\n",
        "    if augmentation_level == 'aggressive':\n",
        "        train_transform, val_transform = get_aggressive_augmentation(img_size)\n",
        "        print(\"\\n✓ Using AGGRESSIVE augmentation\")\n",
        "    elif augmentation_level == 'moderate':\n",
        "        train_transform, val_transform = get_moderate_augmentation(img_size)\n",
        "        print(\"\\n✓ Using MODERATE augmentation (RECOMMENDED)\")\n",
        "    else:\n",
        "        train_transform, val_transform = get_basic_augmentation(img_size)\n",
        "        print(\"\\n✓ Using BASIC augmentation\")\n",
        "\n",
        "    # Initialize K-Fold\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # Store results from each fold\n",
        "    fold_results = []\n",
        "    all_fold_histories = []\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"STARTING {n_splits}-FOLD CROSS-VALIDATION\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Perform K-Fold CV\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(all_imgs, all_lbls), 1):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"FOLD {fold}/{n_splits}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        # Split data for this fold\n",
        "        fold_train_imgs = [all_imgs[i] for i in train_idx]\n",
        "        fold_train_lbls = [all_lbls[i] for i in train_idx]\n",
        "        fold_val_imgs = [all_imgs[i] for i in val_idx]\n",
        "        fold_val_lbls = [all_lbls[i] for i in val_idx]\n",
        "\n",
        "        print(f\"Fold {fold} - Train: {len(fold_train_imgs)}, Val: {len(fold_val_imgs)}\")\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_loader, class_weights = create_dataloader(\n",
        "            fold_train_imgs, fold_train_lbls, train_transform,\n",
        "            batch_size, shuffle=True, num_workers=num_workers\n",
        "        )\n",
        "        val_loader, _ = create_dataloader(\n",
        "            fold_val_imgs, fold_val_lbls, val_transform,\n",
        "            batch_size, shuffle=False, num_workers=num_workers\n",
        "        )\n",
        "\n",
        "        # Initialize fresh model for this fold\n",
        "        model = model_class(num_classes=num_classes, dropout=dropout).to(device)\n",
        "\n",
        "        # Training setup\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device), label_smoothing=0.1)\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
        "        scaler = torch.amp.GradScaler('cuda')\n",
        "        early_stopping = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
        "\n",
        "        # Training history for this fold\n",
        "        fold_history = {\n",
        "            'train_loss': [], 'train_acc': [], 'train_time': [],\n",
        "            'val_loss': [], 'val_acc': [], 'val_precision': [],\n",
        "            'val_recall': [], 'val_macro_f1': [], 'val_weighted_f1': []\n",
        "        }\n",
        "\n",
        "        best_val_acc = 0.0\n",
        "        fold_start = time.time()\n",
        "\n",
        "        # Train this fold\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss, train_acc, train_time = train_epoch(\n",
        "                model, train_loader, criterion, optimizer, scaler, device\n",
        "            )\n",
        "\n",
        "            val_metrics = evaluate(model, val_loader, criterion, device, 'val')\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            # Store metrics\n",
        "            fold_history['train_loss'].append(train_loss)\n",
        "            fold_history['train_acc'].append(train_acc)\n",
        "            fold_history['train_time'].append(train_time)\n",
        "            fold_history['val_loss'].append(val_metrics['loss'])\n",
        "            fold_history['val_acc'].append(val_metrics['accuracy'])\n",
        "            fold_history['val_precision'].append(val_metrics['precision'])\n",
        "            fold_history['val_recall'].append(val_metrics['recall'])\n",
        "            fold_history['val_macro_f1'].append(val_metrics['macro_f1'])\n",
        "            fold_history['val_weighted_f1'].append(val_metrics['weighted_f1'])\n",
        "\n",
        "            if val_metrics['accuracy'] > best_val_acc:\n",
        "                best_val_acc = val_metrics['accuracy']\n",
        "                torch.save(model.state_dict(), f'{experiment_name}_fold{fold}_best.pth')\n",
        "\n",
        "            # Early stopping\n",
        "            if early_stopping(val_metrics['accuracy']):\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        fold_time = time.time() - fold_start\n",
        "\n",
        "        # Load best model from this fold\n",
        "        model.load_state_dict(torch.load(f'{experiment_name}_fold{fold}_best.pth'))\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        final_val_metrics = evaluate(model, val_loader, None, device, 'val')\n",
        "\n",
        "        # Store fold results\n",
        "        fold_result = {\n",
        "            'fold': fold,\n",
        "            'accuracy': final_val_metrics['accuracy'],\n",
        "            'precision': final_val_metrics['precision'],\n",
        "            'recall': final_val_metrics['recall'],\n",
        "            'macro_f1': final_val_metrics['macro_f1'],\n",
        "            'weighted_f1': final_val_metrics['weighted_f1'],\n",
        "            'training_time_minutes': fold_time / 60,\n",
        "            'inference_time_ms': final_val_metrics['avg_inference_time_ms']\n",
        "        }\n",
        "        fold_results.append(fold_result)\n",
        "        all_fold_histories.append(fold_history)\n",
        "\n",
        "        print(f\"\\nFold {fold} Results:\")\n",
        "        print(f\"  Accuracy: {final_val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  Weighted F1: {final_val_metrics['weighted_f1']:.4f}\")\n",
        "        print(f\"  Training Time: {fold_time/60:.2f} minutes\")\n",
        "\n",
        "    # Calculate average results across all folds\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"CROSS-VALIDATION SUMMARY\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    avg_results = {\n",
        "        'model': model_name,\n",
        "        'cv_method': f'{n_splits}-Fold',\n",
        "        'accuracy_mean': np.mean([r['accuracy'] for r in fold_results]),\n",
        "        'accuracy_std': np.std([r['accuracy'] for r in fold_results]),\n",
        "        'precision_mean': np.mean([r['precision'] for r in fold_results]),\n",
        "        'precision_std': np.std([r['precision'] for r in fold_results]),\n",
        "        'recall_mean': np.mean([r['recall'] for r in fold_results]),\n",
        "        'recall_std': np.std([r['recall'] for r in fold_results]),\n",
        "        'macro_f1_mean': np.mean([r['macro_f1'] for r in fold_results]),\n",
        "        'macro_f1_std': np.std([r['macro_f1'] for r in fold_results]),\n",
        "        'weighted_f1_mean': np.mean([r['weighted_f1'] for r in fold_results]),\n",
        "        'weighted_f1_std': np.std([r['weighted_f1'] for r in fold_results]),\n",
        "        'training_time_minutes_mean': np.mean([r['training_time_minutes'] for r in fold_results]),\n",
        "        'inference_time_ms_mean': np.mean([r['inference_time_ms'] for r in fold_results]),\n",
        "        'fold_results': fold_results\n",
        "    }\n",
        "\n",
        "    print(\"Cross-Validation Results (Mean ± Std):\")\n",
        "    print(f\"  Accuracy:     {avg_results['accuracy_mean']:.4f} ± {avg_results['accuracy_std']:.4f}\")\n",
        "    print(f\"  Precision:    {avg_results['precision_mean']:.4f} ± {avg_results['precision_std']:.4f}\")\n",
        "    print(f\"  Recall:       {avg_results['recall_mean']:.4f} ± {avg_results['recall_std']:.4f}\")\n",
        "    print(f\"  Macro F1:     {avg_results['macro_f1_mean']:.4f} ± {avg_results['macro_f1_std']:.4f}\")\n",
        "    print(f\"  Weighted F1:  {avg_results['weighted_f1_mean']:.4f} ± {avg_results['weighted_f1_std']:.4f}\")\n",
        "    print(f\"  Training Time: {avg_results['training_time_minutes_mean']:.2f} minutes per fold\")\n",
        "    print(f\"  Inference Time: {avg_results['inference_time_ms_mean']:.2f} ms per image\")\n",
        "\n",
        "    print(\"\\nPer-Fold Results:\")\n",
        "    for fr in fold_results:\n",
        "        print(f\"  Fold {fr['fold']}: Acc={fr['accuracy']:.4f}, F1={fr['weighted_f1']:.4f}\")\n",
        "\n",
        "    # Save results\n",
        "    with open(f'{experiment_name}_cv_results.json', 'w') as f:\n",
        "        json.dump(avg_results, f, indent=4)\n",
        "    print(f\"\\n✓ Saved: {experiment_name}_cv_results.json\")\n",
        "\n",
        "    # Plot CV results\n",
        "    plot_cv_results(fold_results, experiment_name)\n",
        "\n",
        "    return avg_results, all_fold_histories\n",
        "\n",
        "\n",
        "def plot_cv_results(fold_results, experiment_name):\n",
        "    \"\"\"Plot cross-validation results across folds\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    fig.suptitle(f'{experiment_name.upper()} - Cross-Validation Results', fontsize=14, fontweight='bold')\n",
        "\n",
        "    folds = [f\"Fold {r['fold']}\" for r in fold_results]\n",
        "\n",
        "    # Accuracy and F1 per fold\n",
        "    ax = axes[0]\n",
        "    x = np.arange(len(folds))\n",
        "    width = 0.35\n",
        "\n",
        "    accuracies = [r['accuracy'] for r in fold_results]\n",
        "    f1_scores = [r['weighted_f1'] for r in fold_results]\n",
        "\n",
        "    bars1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='#4ECDC4', alpha=0.8)\n",
        "    bars2 = ax.bar(x + width/2, f1_scores, width, label='Weighted F1', color='#FF6B6B', alpha=0.8)\n",
        "\n",
        "    ax.set_xlabel('Fold')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Accuracy and F1 Score per Fold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(folds)\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    ax.set_ylim([0, 1.0])\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "               f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "               f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    # Summary statistics\n",
        "    ax = axes[1]\n",
        "    ax.axis('off')\n",
        "\n",
        "    mean_acc = np.mean(accuracies)\n",
        "    std_acc = np.std(accuracies)\n",
        "    mean_f1 = np.mean(f1_scores)\n",
        "    std_f1 = np.std(f1_scores)\n",
        "\n",
        "    summary_data = [\n",
        "        ['Metric', 'Mean ± Std'],\n",
        "        ['Accuracy', f'{mean_acc:.4f} ± {std_acc:.4f}'],\n",
        "        ['Weighted F1', f'{mean_f1:.4f} ± {std_f1:.4f}'],\n",
        "        ['', ''],\n",
        "        ['Best Fold', f\"Fold {fold_results[np.argmax(accuracies)]['fold']}\"],\n",
        "        ['Best Accuracy', f\"{max(accuracies):.4f}\"],\n",
        "        ['Worst Fold', f\"Fold {fold_results[np.argmin(accuracies)]['fold']}\"],\n",
        "        ['Worst Accuracy', f\"{min(accuracies):.4f}\"],\n",
        "    ]\n",
        "\n",
        "    table = ax.table(cellText=summary_data, cellLoc='left', loc='center', colWidths=[0.5, 0.5])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1, 2.5)\n",
        "    table[(0, 0)].set_facecolor('#4CAF50')\n",
        "    table[(0, 1)].set_facecolor('#4CAF50')\n",
        "    table[(0, 0)].set_text_props(weight='bold', color='white')\n",
        "    table[(0, 1)].set_text_props(weight='bold', color='white')\n",
        "    ax.set_title('Summary Statistics', fontweight='bold', pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{experiment_name}_cv_results.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved: {experiment_name}_cv_results.png\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Main Execution\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"OPTIMIZED PLANT DISEASE CLASSIFICATION - ALL MODELS FROM SCRATCH\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # ==================== CONFIGURATION ====================\n",
        "    DATASET1_DIR = \"/content/drive/MyDrive/splitted_dataset\"\n",
        "    DATASET2_DIR = \"/content/drive/MyDrive/PlantDoc-Dataset-master_experiment2\"\n",
        "\n",
        "    # OPTIMIZED SETTINGS\n",
        "    EXP1_EPOCHS = 30\n",
        "    EXP1_BATCH_SIZE = 64  # Increased from 32 for faster training\n",
        "    EXP1_LR = 0.0001\n",
        "    EXP1_NUM_WORKERS = 6  # Adjust based on CPU cores\n",
        "\n",
        "    EXP2_EPOCHS = 50\n",
        "    EXP2_BATCH_SIZE = 32  # Increased from 16\n",
        "    EXP2_LR = 0.0001\n",
        "    EXP2_NUM_WORKERS = 4\n",
        "\n",
        "    IMG_SIZE = 224\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"OPTIMIZED CONFIGURATION\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Experiment 1 - Epochs: {EXP1_EPOCHS}, Batch: {EXP1_BATCH_SIZE}, LR: {EXP1_LR}, Workers: {EXP1_NUM_WORKERS}\")\n",
        "    print(f\"Experiment 2 - Epochs: {EXP2_EPOCHS}, Batch: {EXP2_BATCH_SIZE}, LR: {EXP2_LR}, Workers: {EXP2_NUM_WORKERS}\")\n",
        "    print(f\"Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "    print(\"\\nKEY OPTIMIZATIONS:\")\n",
        "    print(\"✓ Removed WeightedRandomSampler (using class weights in loss instead)\")\n",
        "    print(\"✓ Increased batch sizes for faster training\")\n",
        "    print(\"✓ Added persistent_workers and prefetch_factor\")\n",
        "    print(\"✓ Using moderate augmentation by default (balanced speed/performance)\")\n",
        "    print(\"✓ Optimized data loading with more workers\")\n",
        "    print(\"✓ Using set_to_none=True in optimizer.zero_grad()\")\n",
        "    print(\"✓ Added non_blocking=True for GPU transfers\")\n",
        "\n",
        "    # ==================== EXPERIMENT 1: VGG13 ====================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EXPERIMENT 1: VGG13 ON LARGE DATASET\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    temp_imgs, temp_lbls, temp_classes = load_dataset_type1(DATASET1_DIR, split='train')\n",
        "    num_classes_exp1 = len(temp_classes)\n",
        "    print(f\"\\n✓ Auto-detected {num_classes_exp1} classes\")\n",
        "\n",
        "    model_vgg13_exp1 = VGG13(num_classes=num_classes_exp1, dropout=0.5)\n",
        "    exp1_results, exp1_history = train_model(\n",
        "        model=model_vgg13_exp1,\n",
        "        model_name='vgg13',\n",
        "        dataset_dir=DATASET1_DIR,\n",
        "        dataset_type=1,\n",
        "        num_epochs=EXP1_EPOCHS,\n",
        "        batch_size=EXP1_BATCH_SIZE,\n",
        "        lr=EXP1_LR,\n",
        "        img_size=IMG_SIZE,\n",
        "        experiment_name='exp1_vgg13_optimized',\n",
        "        augmentation_level='moderate',  # Changed from aggressive\n",
        "        num_workers=EXP1_NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ==================== EXPERIMENT 2: VGG13 WITH CROSS-VALIDATION ====================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EXPERIMENT 2A: VGG13 WITH 5-FOLD CROSS-VALIDATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    temp_imgs2, temp_lbls2, temp_classes2 = load_dataset_type2(DATASET2_DIR, split='train')\n",
        "    num_classes_exp2 = len(temp_classes2)\n",
        "    print(f\"\\n✓ Auto-detected {num_classes_exp2} classes\")\n",
        "\n",
        "    exp2a_results_cv, exp2a_histories_cv = train_model_cv(\n",
        "        model_class=VGG13,\n",
        "        model_name='vgg13',\n",
        "        dataset_dir=DATASET2_DIR,\n",
        "        dataset_type=2,\n",
        "        num_epochs=EXP2_EPOCHS,\n",
        "        batch_size=EXP2_BATCH_SIZE,\n",
        "        lr=EXP2_LR,\n",
        "        img_size=IMG_SIZE,\n",
        "        experiment_name='exp2_vgg13_cv_optimized',\n",
        "        augmentation_level='moderate',\n",
        "        n_splits=5,\n",
        "        dropout=0.5,\n",
        "        num_workers=EXP2_NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ==================== EXPERIMENT 2: RESNET18 WITH CROSS-VALIDATION ====================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EXPERIMENT 2B: RESNET18 WITH 5-FOLD CROSS-VALIDATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    exp2b_results_cv, exp2b_histories_cv = train_model_cv(\n",
        "        model_class=ResNet18,\n",
        "        model_name='resnet18',\n",
        "        dataset_dir=DATASET2_DIR,\n",
        "        dataset_type=2,\n",
        "        num_epochs=EXP2_EPOCHS,\n",
        "        batch_size=EXP2_BATCH_SIZE,\n",
        "        lr=EXP2_LR,\n",
        "        img_size=IMG_SIZE,\n",
        "        experiment_name='exp2_resnet18_cv_optimized',\n",
        "        augmentation_level='moderate',\n",
        "        n_splits=5,\n",
        "        dropout=0.5,\n",
        "        num_workers=EXP2_NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ==================== COMPARISON ====================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ALL EXPERIMENTS COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Metric': ['Accuracy', 'Precision', 'Recall', 'Macro F1', 'Weighted F1',\n",
        "                   'Training Time (min)', 'Inference Time (ms)'],\n",
        "        'Exp1 (VGG13 Large)': [\n",
        "            f\"{exp1_results['accuracy']:.4f}\",\n",
        "            f\"{exp1_results['precision']:.4f}\",\n",
        "            f\"{exp1_results['recall']:.4f}\",\n",
        "            f\"{exp1_results['macro_f1']:.4f}\",\n",
        "            f\"{exp1_results['weighted_f1']:.4f}\",\n",
        "            f\"{exp1_results['training_time_minutes']:.2f}\",\n",
        "            f\"{exp1_results['inference_time_ms']:.2f}\"\n",
        "        ],\n",
        "        'Exp2 (VGG13 CV)': [\n",
        "            f\"{exp2a_results_cv['accuracy_mean']:.4f} ± {exp2a_results_cv['accuracy_std']:.4f}\",\n",
        "            f\"{exp2a_results_cv['precision_mean']:.4f} ± {exp2a_results_cv['precision_std']:.4f}\",\n",
        "            f\"{exp2a_results_cv['recall_mean']:.4f} ± {exp2a_results_cv['recall_std']:.4f}\",\n",
        "            f\"{exp2a_results_cv['macro_f1_mean']:.4f} ± {exp2a_results_cv['macro_f1_std']:.4f}\",\n",
        "            f\"{exp2a_results_cv['weighted_f1_mean']:.4f} ± {exp2a_results_cv['weighted_f1_std']:.4f}\",\n",
        "            f\"{exp2a_results_cv['training_time_minutes_mean']:.2f}\",\n",
        "            f\"{exp2a_results_cv['inference_time_ms_mean']:.2f}\"\n",
        "        ],\n",
        "        'Exp2 (ResNet18 CV)': [\n",
        "            f\"{exp2b_results_cv['accuracy_mean']:.4f} ± {exp2b_results_cv['accuracy_std']:.4f}\",\n",
        "            f\"{exp2b_results_cv['precision_mean']:.4f} ± {exp2b_results_cv['precision_std']:.4f}\",\n",
        "            f\"{exp2b_results_cv['recall_mean']:.4f} ± {exp2b_results_cv['recall_std']:.4f}\",\n",
        "            f\"{exp2b_results_cv['macro_f1_mean']:.4f} ± {exp2b_results_cv['macro_f1_std']:.4f}\",\n",
        "            f\"{exp2b_results_cv['weighted_f1_mean']:.4f} ± {exp2b_results_cv['weighted_f1_std']:.4f}\",\n",
        "            f\"{exp2b_results_cv['training_time_minutes_mean']:.2f}\",\n",
        "            f\"{exp2b_results_cv['inference_time_ms_mean']:.2f}\"\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    print(\"\\n\", comparison_df.to_string(index=False))\n",
        "    comparison_df.to_csv('all_experiments_comparison_optimized.csv', index=False)\n",
        "    print(\"\\n✓ Saved: all_experiments_comparison_optimized.csv\")\n",
        "\n",
        "    # Comparison plot\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    fig.suptitle('All Experiments Comparison (Optimized)', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Accuracy comparison\n",
        "    ax = axes[0]\n",
        "    models = ['Exp1\\n(VGG13\\nLarge)', 'Exp2\\n(VGG13\\nCV)', 'Exp2\\n(ResNet18\\nCV)']\n",
        "    accuracies = [\n",
        "        exp1_results['accuracy'],\n",
        "        exp2a_results_cv['accuracy_mean'],\n",
        "        exp2b_results_cv['accuracy_mean']\n",
        "    ]\n",
        "    stds = [0, exp2a_results_cv['accuracy_std'], exp2b_results_cv['accuracy_std']]\n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "\n",
        "    bars = ax.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "    ax.errorbar(models[1:], accuracies[1:], yerr=stds[1:], fmt='none', ecolor='black',\n",
        "                capsize=5, capthick=2, linewidth=2)\n",
        "\n",
        "    ax.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax.set_title('Accuracy Comparison', fontsize=13, fontweight='bold')\n",
        "    ax.set_ylim([0, 1.0])\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
        "        height = bar.get_height()\n",
        "        if i == 0:\n",
        "            label = f'{acc:.4f}'\n",
        "        else:\n",
        "            label = f'{acc:.4f}\\n±{stds[i]:.4f}'\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "               label, ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "    # F1 comparison\n",
        "    ax = axes[1]\n",
        "    f1_scores = [\n",
        "        exp1_results['weighted_f1'],\n",
        "        exp2a_results_cv['weighted_f1_mean'],\n",
        "        exp2b_results_cv['weighted_f1_mean']\n",
        "    ]\n",
        "    f1_stds = [0, exp2a_results_cv['weighted_f1_std'], exp2b_results_cv['weighted_f1_std']]\n",
        "\n",
        "    bars = ax.bar(models, f1_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "    ax.errorbar(models[1:], f1_scores[1:], yerr=f1_stds[1:], fmt='none', ecolor='black',\n",
        "                capsize=5, capthick=2, linewidth=2)\n",
        "\n",
        "    ax.set_ylabel('Weighted F1 Score', fontsize=12)\n",
        "    ax.set_title('F1 Score Comparison', fontsize=13, fontweight='bold')\n",
        "    ax.set_ylim([0, 1.0])\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for i, (bar, f1) in enumerate(zip(bars, f1_scores)):\n",
        "        height = bar.get_height()\n",
        "        if i == 0:\n",
        "            label = f'{f1:.4f}'\n",
        "        else:\n",
        "            label = f'{f1:.4f}\\n±{f1_stds[i]:.4f}'\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "               label, ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('all_experiments_comparison_optimized.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"✓ Saved: all_experiments_comparison_optimized.png\")\n",
        "\n",
        "    # Best model for Experiment 2\n",
        "    if exp2b_results_cv['accuracy_mean'] > exp2a_results_cv['accuracy_mean']:\n",
        "        best_model_name = 'ResNet18'\n",
        "        best_results = exp2b_results_cv\n",
        "    else:\n",
        "        best_model_name = 'VGG13'\n",
        "        best_results = exp2a_results_cv\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"🏆 BEST MODEL FOR EXPERIMENT 2 (CV): {best_model_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"  Accuracy:     {best_results['accuracy_mean']:.4f} ± {best_results['accuracy_std']:.4f}\")\n",
        "    print(f\"  Weighted F1:  {best_results['weighted_f1_mean']:.4f} ± {best_results['weighted_f1_std']:.4f}\")\n",
        "    print(f\"  Training Time: {best_results['training_time_minutes_mean']:.2f} minutes per fold\")\n",
        "    print(f\"  Inference Time: {best_results['inference_time_ms_mean']:.2f} ms per image\")\n",
        "\n",
        "    # Speed improvement summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"OPTIMIZATION SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"✓ Removed WeightedRandomSampler → 3-5x faster data loading\")\n",
        "    print(\"✓ Increased batch sizes → Fewer iterations per epoch\")\n",
        "    print(\"✓ Persistent workers → No worker restart between epochs\")\n",
        "    print(\"✓ Moderate augmentation → 2-3x faster augmentation pipeline\")\n",
        "    print(\"✓ Expected speedup: 5-10x faster training (78min → 8-15min per epoch)\")\n",
        "    print(\"\\n✓ Class imbalance still handled via weighted CrossEntropyLoss\")\n",
        "    print(\"✓ Model performance should be similar or better\")\n",
        "    print(\"✓ Memory usage reduced\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"KEY FEATURES\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"✓ Experiment 1: Single train/val/test split with moderate augmentation\")\n",
        "    print(\"✓ Experiment 2: 5-Fold Cross-Validation for robust evaluation\")\n",
        "    print(\"✓ Inverse frequency class weights in loss function\")\n",
        "    print(\"✓ Early stopping prevents overfitting\")\n",
        "    print(\"✓ Mixed precision training (AMP)\")\n",
        "    print(\"✓ Gradient clipping for stability\")\n",
        "    print(\"✓ Cosine annealing learning rate scheduler\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ALL EXPERIMENTS COMPLETED!\")\n",
        "    print(\"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTqSGrprAxaK",
        "outputId": "b6b668ef-1662-41b7-ad3c-6a9ba28d3aef"
      },
      "id": "GTqSGrprAxaK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "======================================================================\n",
            "OPTIMIZED PLANT DISEASE CLASSIFICATION - ALL MODELS FROM SCRATCH\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "OPTIMIZED CONFIGURATION\n",
            "======================================================================\n",
            "Experiment 1 - Epochs: 30, Batch: 64, LR: 0.0001, Workers: 6\n",
            "Experiment 2 - Epochs: 50, Batch: 32, LR: 0.0001, Workers: 4\n",
            "Image Size: 224x224\n",
            "\n",
            "KEY OPTIMIZATIONS:\n",
            "✓ Removed WeightedRandomSampler (using class weights in loss instead)\n",
            "✓ Increased batch sizes for faster training\n",
            "✓ Added persistent_workers and prefetch_factor\n",
            "✓ Using moderate augmentation by default (balanced speed/performance)\n",
            "✓ Optimized data loading with more workers\n",
            "✓ Using set_to_none=True in optimizer.zero_grad()\n",
            "✓ Added non_blocking=True for GPU transfers\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT 1: VGG13 ON LARGE DATASET\n",
            "======================================================================\n",
            "\n",
            "✓ Auto-detected 12 classes\n",
            "\n",
            "======================================================================\n",
            "EXP1_VGG13_OPTIMIZED: VGG13 FROM SCRATCH\n",
            "======================================================================\n",
            "\n",
            "Dataset Structure: dataset/class/train | validation | test\n",
            "\n",
            "Loading training data from: /content/drive/MyDrive/splitted_dataset\n",
            "\n",
            "======================================================================\n",
            "Training Set Analysis\n",
            "======================================================================\n",
            "Total images: 15829\n",
            "Number of classes: 12\n",
            "\n",
            "Class Distribution:\n",
            "  Pepper__bell___Bacterial_spot: 907 images (5.7%)\n",
            "  Pepper__bell___healthy: 1330 images (8.4%)\n",
            "  Potato___Early_blight: 913 images (5.8%)\n",
            "  Potato___Late_blight: 904 images (5.7%)\n",
            "  Tomato_Bacterial_spot: 1936 images (12.2%)\n",
            "  Tomato_Early_blight: 911 images (5.8%)\n",
            "  Tomato_Late_blight: 1729 images (10.9%)\n",
            "  Tomato_Leaf_Mold: 861 images (5.4%)\n",
            "  Tomato_Septoria_leaf_spot: 1614 images (10.2%)\n",
            "  Tomato__Tomato_YellowLeaf__Curl_Virus: 2918 images (18.4%)\n",
            "  Tomato__Tomato_mosaic_virus: 346 images (2.2%)\n",
            "  Tomato_healthy: 1460 images (9.2%)\n",
            "\n",
            "Imbalance Ratio: 8.43\n",
            "⚠️  Severe class imbalance detected! Will use class weights in loss.\n",
            "\n",
            "Loading validation data from: /content/drive/MyDrive/splitted_dataset\n",
            "\n",
            "======================================================================\n",
            "Validation Set Analysis\n",
            "======================================================================\n",
            "Total images: 4799\n",
            "Number of classes: 12\n",
            "\n",
            "Class Distribution:\n",
            "  Pepper__bell___Bacterial_spot: 278 images (5.8%)\n",
            "  Pepper__bell___healthy: 399 images (8.3%)\n",
            "  Potato___Early_blight: 274 images (5.7%)\n",
            "  Potato___Late_blight: 274 images (5.7%)\n",
            "  Tomato_Bacterial_spot: 592 images (12.3%)\n",
            "  Tomato_Early_blight: 279 images (5.8%)\n",
            "  Tomato_Late_blight: 524 images (10.9%)\n",
            "  Tomato_Leaf_Mold: 258 images (5.4%)\n",
            "  Tomato_Septoria_leaf_spot: 487 images (10.1%)\n",
            "  Tomato__Tomato_YellowLeaf__Curl_Virus: 885 images (18.4%)\n",
            "  Tomato__Tomato_mosaic_virus: 102 images (2.1%)\n",
            "  Tomato_healthy: 447 images (9.3%)\n",
            "\n",
            "Imbalance Ratio: 8.68\n",
            "⚠️  Severe class imbalance detected! Will use class weights in loss.\n",
            "\n",
            "Loading test data from: /content/drive/MyDrive/splitted_dataset\n",
            "\n",
            "======================================================================\n",
            "Test Set Analysis\n",
            "======================================================================\n",
            "Total images: 1095\n",
            "Number of classes: 12\n",
            "\n",
            "Class Distribution:\n",
            "  Pepper__bell___Bacterial_spot: 71 images (6.5%)\n",
            "  Pepper__bell___healthy: 61 images (5.6%)\n",
            "  Potato___Early_blight: 116 images (10.6%)\n",
            "  Potato___Late_blight: 105 images (9.6%)\n",
            "  Tomato_Bacterial_spot: 110 images (10.0%)\n",
            "  Tomato_Early_blight: 88 images (8.0%)\n",
            "  Tomato_Late_blight: 111 images (10.1%)\n",
            "  Tomato_Leaf_Mold: 91 images (8.3%)\n",
            "  Tomato_Septoria_leaf_spot: 150 images (13.7%)\n",
            "  Tomato__Tomato_YellowLeaf__Curl_Virus: 75 images (6.8%)\n",
            "  Tomato__Tomato_mosaic_virus: 54 images (4.9%)\n",
            "  Tomato_healthy: 63 images (5.8%)\n",
            "\n",
            "Imbalance Ratio: 2.78\n",
            "\n",
            "✓ Using MODERATE augmentation (RECOMMENDED for best speed/performance)\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 15829\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=907, Weight=1.1074\n",
            "  Class 1: Count=1330, Weight=0.7552\n",
            "  Class 2: Count=913, Weight=1.1001\n",
            "  Class 3: Count=904, Weight=1.1111\n",
            "  Class 4: Count=1936, Weight=0.5188\n",
            "  Class 5: Count=911, Weight=1.1025\n",
            "  Class 6: Count=1729, Weight=0.5809\n",
            "  Class 7: Count=861, Weight=1.1666\n",
            "  Class 8: Count=1614, Weight=0.6223\n",
            "  Class 9: Count=2918, Weight=0.3442\n",
            "  Class 10: Count=346, Weight=2.9029\n",
            "  Class 11: Count=1460, Weight=0.6880\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 4799\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=278, Weight=1.0877\n",
            "  Class 1: Count=399, Weight=0.7579\n",
            "  Class 2: Count=274, Weight=1.1036\n",
            "  Class 3: Count=274, Weight=1.1036\n",
            "  Class 4: Count=592, Weight=0.5108\n",
            "  Class 5: Count=279, Weight=1.0838\n",
            "  Class 6: Count=524, Weight=0.5771\n",
            "  Class 7: Count=258, Weight=1.1720\n",
            "  Class 8: Count=487, Weight=0.6209\n",
            "  Class 9: Count=885, Weight=0.3417\n",
            "  Class 10: Count=102, Weight=2.9645\n",
            "  Class 11: Count=447, Weight=0.6765\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "TRAINING\n",
            "======================================================================\n",
            "Total batches per epoch: 248\n",
            "Estimated epoch time: ~2.1 minutes\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [25:48<00:00,  6.24s/it, loss=2.25, acc=34.2]\n",
            "Val: 100%|██████████| 75/75 [11:04<00:00,  8.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.2496, Train Acc: 34.20%, Time: 25.81 min\n",
            "Val Acc: 0.6887, Val F1: 0.6772\n",
            "✓ Saved best model! Acc: 0.6887\n",
            "\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=1.59, acc=60.2]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.5885, Train Acc: 60.23%, Time: 2.14 min\n",
            "Val Acc: 0.8506, Val F1: 0.8519\n",
            "✓ Saved best model! Acc: 0.8506\n",
            "\n",
            "Epoch 3/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.94it/s, loss=1.32, acc=72.7]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.3228, Train Acc: 72.71%, Time: 2.13 min\n",
            "Val Acc: 0.8964, Val F1: 0.8988\n",
            "✓ Saved best model! Acc: 0.8964\n",
            "\n",
            "Epoch 4/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:07<00:00,  1.94it/s, loss=1.17, acc=80.1]\n",
            "Val: 100%|██████████| 75/75 [00:21<00:00,  3.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1688, Train Acc: 80.13%, Time: 2.13 min\n",
            "Val Acc: 0.9264, Val F1: 0.9264\n",
            "✓ Saved best model! Acc: 0.9264\n",
            "\n",
            "Epoch 5/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=1.08, acc=84]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0830, Train Acc: 84.05%, Time: 2.14 min\n",
            "Val Acc: 0.9606, Val F1: 0.9607\n",
            "✓ Saved best model! Acc: 0.9606\n",
            "\n",
            "Epoch 6/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:11<00:00,  1.88it/s, loss=1, acc=87.4]\n",
            "Val: 100%|██████████| 75/75 [00:21<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0034, Train Acc: 87.45%, Time: 2.19 min\n",
            "Val Acc: 0.9552, Val F1: 0.9553\n",
            "\n",
            "Epoch 7/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:07<00:00,  1.94it/s, loss=0.953, acc=89.3]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9531, Train Acc: 89.25%, Time: 2.13 min\n",
            "Val Acc: 0.9708, Val F1: 0.9708\n",
            "✓ Saved best model! Acc: 0.9708\n",
            "\n",
            "Epoch 8/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.92it/s, loss=0.915, acc=91.1]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9151, Train Acc: 91.13%, Time: 2.15 min\n",
            "Val Acc: 0.9721, Val F1: 0.9721\n",
            "✓ Saved best model! Acc: 0.9721\n",
            "\n",
            "Epoch 9/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=0.888, acc=92.1]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8877, Train Acc: 92.12%, Time: 2.14 min\n",
            "Val Acc: 0.9827, Val F1: 0.9827\n",
            "✓ Saved best model! Acc: 0.9827\n",
            "\n",
            "Epoch 10/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:13<00:00,  1.85it/s, loss=0.864, acc=92.9]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8645, Train Acc: 92.94%, Time: 2.23 min\n",
            "Val Acc: 0.9850, Val F1: 0.9850\n",
            "✓ Saved best model! Acc: 0.9850\n",
            "\n",
            "Epoch 11/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=0.99, acc=87.7]\n",
            "Val: 100%|██████████| 75/75 [00:21<00:00,  3.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9904, Train Acc: 87.72%, Time: 2.14 min\n",
            "Val Acc: 0.9433, Val F1: 0.9447\n",
            "\n",
            "Epoch 12/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.94it/s, loss=0.963, acc=89.2]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9634, Train Acc: 89.18%, Time: 2.14 min\n",
            "Val Acc: 0.9477, Val F1: 0.9488\n",
            "\n",
            "Epoch 13/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.94it/s, loss=0.94, acc=89.9]\n",
            "Val: 100%|██████████| 75/75 [00:21<00:00,  3.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9396, Train Acc: 89.92%, Time: 2.13 min\n",
            "Val Acc: 0.9687, Val F1: 0.9689\n",
            "\n",
            "Epoch 14/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:07<00:00,  1.94it/s, loss=0.906, acc=91.2]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9060, Train Acc: 91.20%, Time: 2.13 min\n",
            "Val Acc: 0.9723, Val F1: 0.9724\n",
            "\n",
            "Epoch 15/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=0.891, acc=91.9]\n",
            "Val: 100%|██████████| 75/75 [00:21<00:00,  3.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8911, Train Acc: 91.90%, Time: 2.14 min\n",
            "Val Acc: 0.9381, Val F1: 0.9370\n",
            "\n",
            "Epoch 16/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.94it/s, loss=0.874, acc=92.3]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8736, Train Acc: 92.35%, Time: 2.13 min\n",
            "Val Acc: 0.9879, Val F1: 0.9879\n",
            "✓ Saved best model! Acc: 0.9879\n",
            "\n",
            "Epoch 17/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=0.849, acc=93.3]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8488, Train Acc: 93.28%, Time: 2.14 min\n",
            "Val Acc: 0.9694, Val F1: 0.9694\n",
            "\n",
            "Epoch 18/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=0.843, acc=93.6]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8427, Train Acc: 93.60%, Time: 2.14 min\n",
            "Val Acc: 0.9852, Val F1: 0.9852\n",
            "\n",
            "Epoch 19/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.94it/s, loss=0.821, acc=94.2]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8210, Train Acc: 94.24%, Time: 2.14 min\n",
            "Val Acc: 0.9852, Val F1: 0.9852\n",
            "\n",
            "Epoch 20/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=0.81, acc=94.6]\n",
            "Val: 100%|██████████| 75/75 [00:26<00:00,  2.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8104, Train Acc: 94.65%, Time: 2.14 min\n",
            "Val Acc: 0.9817, Val F1: 0.9816\n",
            "\n",
            "Epoch 21/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.92it/s, loss=0.793, acc=95.2]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7932, Train Acc: 95.22%, Time: 2.15 min\n",
            "Val Acc: 0.9879, Val F1: 0.9879\n",
            "\n",
            "Epoch 22/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=0.778, acc=95.7]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7780, Train Acc: 95.70%, Time: 2.14 min\n",
            "Val Acc: 0.9908, Val F1: 0.9908\n",
            "✓ Saved best model! Acc: 0.9908\n",
            "\n",
            "Epoch 23/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=0.767, acc=96.1]\n",
            "Val: 100%|██████████| 75/75 [00:23<00:00,  3.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7666, Train Acc: 96.11%, Time: 2.14 min\n",
            "Val Acc: 0.9958, Val F1: 0.9958\n",
            "✓ Saved best model! Acc: 0.9958\n",
            "\n",
            "Epoch 24/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:09<00:00,  1.92it/s, loss=0.754, acc=96.6]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7539, Train Acc: 96.63%, Time: 2.15 min\n",
            "Val Acc: 0.9933, Val F1: 0.9933\n",
            "\n",
            "Epoch 25/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:07<00:00,  1.94it/s, loss=0.75, acc=96.8]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7505, Train Acc: 96.78%, Time: 2.13 min\n",
            "Val Acc: 0.9962, Val F1: 0.9962\n",
            "✓ Saved best model! Acc: 0.9962\n",
            "\n",
            "Epoch 26/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=0.743, acc=97.1]\n",
            "Val: 100%|██████████| 75/75 [00:21<00:00,  3.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7430, Train Acc: 97.09%, Time: 2.14 min\n",
            "Val Acc: 0.9935, Val F1: 0.9935\n",
            "\n",
            "Epoch 27/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:07<00:00,  1.94it/s, loss=0.735, acc=97.3]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7353, Train Acc: 97.34%, Time: 2.13 min\n",
            "Val Acc: 0.9958, Val F1: 0.9958\n",
            "\n",
            "Epoch 28/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=0.735, acc=97.4]\n",
            "Val: 100%|██████████| 75/75 [00:21<00:00,  3.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7346, Train Acc: 97.38%, Time: 2.14 min\n",
            "Val Acc: 0.9962, Val F1: 0.9962\n",
            "\n",
            "Epoch 29/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=0.734, acc=97.5]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7335, Train Acc: 97.48%, Time: 2.14 min\n",
            "Val Acc: 0.9969, Val F1: 0.9969\n",
            "✓ Saved best model! Acc: 0.9969\n",
            "\n",
            "Epoch 30/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s, loss=0.726, acc=97.8]\n",
            "Val: 100%|██████████| 75/75 [00:22<00:00,  3.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7263, Train Acc: 97.80%, Time: 2.14 min\n",
            "Val Acc: 0.9956, Val F1: 0.9956\n",
            "\n",
            "======================================================================\n",
            "TESTING\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 1095\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=71, Weight=1.1781\n",
            "  Class 1: Count=61, Weight=1.3712\n",
            "  Class 2: Count=116, Weight=0.7211\n",
            "  Class 3: Count=105, Weight=0.7966\n",
            "  Class 4: Count=110, Weight=0.7604\n",
            "  Class 5: Count=88, Weight=0.9505\n",
            "  Class 6: Count=111, Weight=0.7535\n",
            "  Class 7: Count=91, Weight=0.9192\n",
            "  Class 8: Count=150, Weight=0.5576\n",
            "  Class 9: Count=75, Weight=1.1152\n",
            "  Class 10: Count=54, Weight=1.5489\n",
            "  Class 11: Count=63, Weight=1.3277\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 18/18 [03:30<00:00, 11.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "EXP1_VGG13_OPTIMIZED FINAL RESULTS\n",
            "======================================================================\n",
            "Model:                 VGG13\n",
            "Accuracy:              0.1863\n",
            "Precision:             0.1995\n",
            "Recall:                0.1863\n",
            "Macro F1:              0.1408\n",
            "Weighted F1:           0.1436\n",
            "Training Time:         6811.19 seconds (113.52 minutes)\n",
            "Inference Time:        4.32 ms per image\n",
            "======================================================================\n",
            "\n",
            "✓ Saved: exp1_vgg13_optimized_results.json\n",
            "✓ Saved: exp1_vgg13_optimized_training_history.png\n",
            "✓ Saved: exp1_vgg13_optimized_confusion_matrix.png\n",
            "\n",
            "Per-Class Classification Report:\n",
            "                                       precision    recall  f1-score   support\n",
            "\n",
            "        Pepper__bell___Bacterial_spot     0.1520    0.2676    0.1939        71\n",
            "               Pepper__bell___healthy     0.1515    0.0820    0.1064        61\n",
            "                Potato___Early_blight     0.1731    0.2328    0.1985       116\n",
            "                 Potato___Late_blight     0.2500    0.0286    0.0513       105\n",
            "                Tomato_Bacterial_spot     0.0909    0.0091    0.0165       110\n",
            "                  Tomato_Early_blight     0.1583    0.5000    0.2404        88\n",
            "                   Tomato_Late_blight     0.2072    0.6216    0.3108       111\n",
            "                     Tomato_Leaf_Mold     0.1594    0.1209    0.1375        91\n",
            "            Tomato_Septoria_leaf_spot     0.2143    0.0600    0.0938       150\n",
            "Tomato__Tomato_YellowLeaf__Curl_Virus     0.5000    0.1733    0.2574        75\n",
            "          Tomato__Tomato_mosaic_virus     0.0000    0.0000    0.0000        54\n",
            "                       Tomato_healthy     0.3333    0.0476    0.0833        63\n",
            "\n",
            "                             accuracy                         0.1863      1095\n",
            "                            macro avg     0.1992    0.1786    0.1408      1095\n",
            "                         weighted avg     0.1995    0.1863    0.1436      1095\n",
            "\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT 2A: VGG13 WITH 5-FOLD CROSS-VALIDATION\n",
            "======================================================================\n",
            "\n",
            "✓ Auto-detected 12 classes\n",
            "\n",
            "======================================================================\n",
            "EXP2_VGG13_CV_OPTIMIZED: VGG13 WITH 5-FOLD CV\n",
            "======================================================================\n",
            "\n",
            "Loading training data from: /content/drive/MyDrive/PlantDoc-Dataset-master_experiment2/train\n",
            "Loading test data from: /content/drive/MyDrive/PlantDoc-Dataset-master_experiment2/test\n",
            "\n",
            "✓ Total samples for CV: 1095\n",
            "\n",
            "======================================================================\n",
            "Complete Dataset for CV Analysis\n",
            "======================================================================\n",
            "Total images: 1095\n",
            "Number of classes: 12\n",
            "\n",
            "Class Distribution:\n",
            "  Pepper__bell___Bacterial_spot: 71 images (6.5%)\n",
            "  Pepper__bell___healthy: 61 images (5.6%)\n",
            "  Potato___Early_blight: 116 images (10.6%)\n",
            "  Potato___Late_blight: 105 images (9.6%)\n",
            "  Tomato_Bacterial_spot: 110 images (10.0%)\n",
            "  Tomato_Early_blight: 88 images (8.0%)\n",
            "  Tomato_Late_blight: 111 images (10.1%)\n",
            "  Tomato_Leaf_Mold: 91 images (8.3%)\n",
            "  Tomato_Septoria_leaf_spot: 150 images (13.7%)\n",
            "  Tomato__Tomato_YellowLeaf__Curl_Virus: 75 images (6.8%)\n",
            "  Tomato__Tomato_mosaic_virus: 54 images (4.9%)\n",
            "  Tomato_healthy: 63 images (5.8%)\n",
            "\n",
            "Imbalance Ratio: 2.78\n",
            "\n",
            "✓ Using MODERATE augmentation (RECOMMENDED)\n",
            "\n",
            "======================================================================\n",
            "STARTING 5-FOLD CROSS-VALIDATION\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "FOLD 1/5\n",
            "======================================================================\n",
            "\n",
            "Fold 1 - Train: 876, Val: 219\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 876\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=56, Weight=1.1947\n",
            "  Class 1: Count=49, Weight=1.3653\n",
            "  Class 2: Count=93, Weight=0.7194\n",
            "  Class 3: Count=84, Weight=0.7964\n",
            "  Class 4: Count=88, Weight=0.7602\n",
            "  Class 5: Count=70, Weight=0.9557\n",
            "  Class 6: Count=89, Weight=0.7517\n",
            "  Class 7: Count=73, Weight=0.9164\n",
            "  Class 8: Count=120, Weight=0.5575\n",
            "  Class 9: Count=60, Weight=1.1150\n",
            "  Class 10: Count=43, Weight=1.5558\n",
            "  Class 11: Count=51, Weight=1.3118\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 219\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=15, Weight=1.1152\n",
            "  Class 1: Count=12, Weight=1.3940\n",
            "  Class 2: Count=23, Weight=0.7273\n",
            "  Class 3: Count=21, Weight=0.7966\n",
            "  Class 4: Count=22, Weight=0.7604\n",
            "  Class 5: Count=18, Weight=0.9293\n",
            "  Class 6: Count=22, Weight=0.7604\n",
            "  Class 7: Count=18, Weight=0.9293\n",
            "  Class 8: Count=30, Weight=0.5576\n",
            "  Class 9: Count=15, Weight=1.1152\n",
            "  Class 10: Count=11, Weight=1.5207\n",
            "  Class 11: Count=12, Weight=1.3940\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 28/28 [02:11<00:00,  4.70s/it, loss=3.41, acc=8.56]\n",
            "Val: 100%|██████████| 7/7 [00:44<00:00,  6.36s/it]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.50it/s, loss=3.07, acc=9.25]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.05it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.47it/s, loss=2.76, acc=11.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.11it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.54it/s, loss=2.77, acc=8.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.96it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.45it/s, loss=2.65, acc=10]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.04it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.57it/s, loss=2.54, acc=10.7]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.59it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.55it/s, loss=2.5, acc=12.7]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.08it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.53it/s, loss=2.48, acc=13.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.07it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.51it/s, loss=2.44, acc=13.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.81it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.45it/s, loss=2.44, acc=14.7]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.07it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.59it/s, loss=2.51, acc=12.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.57it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.56it/s, loss=2.5, acc=11.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.05it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.58it/s, loss=2.5, acc=12.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.43it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.51it/s, loss=2.49, acc=13]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.70it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.49it/s, loss=2.46, acc=13.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.07it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.52it/s, loss=2.45, acc=12.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.08it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.56it/s, loss=2.41, acc=15.6]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.48it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.54it/s, loss=2.42, acc=15]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.12it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.52it/s, loss=2.4, acc=14.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.05it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.55it/s, loss=2.35, acc=17.6]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.56it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.57it/s, loss=2.34, acc=17.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.02it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.45it/s, loss=2.34, acc=17.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.05it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.53it/s, loss=2.32, acc=17.9]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.53it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.59it/s, loss=2.31, acc=18.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.08it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.54it/s, loss=2.28, acc=19.1]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.65it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.58it/s, loss=2.26, acc=20.2]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.07it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.49it/s, loss=2.24, acc=22]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.81it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.54it/s, loss=2.22, acc=22.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.12it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.46it/s, loss=2.26, acc=21]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.04it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.51it/s, loss=2.22, acc=22.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.87it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.48it/s, loss=2.27, acc=19.2]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1 Results:\n",
            "  Accuracy: 0.2100\n",
            "  Weighted F1: 0.1737\n",
            "  Training Time: 14.64 minutes\n",
            "\n",
            "======================================================================\n",
            "FOLD 2/5\n",
            "======================================================================\n",
            "\n",
            "Fold 2 - Train: 876, Val: 219\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 876\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=57, Weight=1.1734\n",
            "  Class 1: Count=48, Weight=1.3935\n",
            "  Class 2: Count=93, Weight=0.7192\n",
            "  Class 3: Count=84, Weight=0.7963\n",
            "  Class 4: Count=88, Weight=0.7601\n",
            "  Class 5: Count=71, Weight=0.9421\n",
            "  Class 6: Count=88, Weight=0.7601\n",
            "  Class 7: Count=73, Weight=0.9163\n",
            "  Class 8: Count=120, Weight=0.5574\n",
            "  Class 9: Count=60, Weight=1.1148\n",
            "  Class 10: Count=43, Weight=1.5555\n",
            "  Class 11: Count=51, Weight=1.3115\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 219\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=14, Weight=1.1955\n",
            "  Class 1: Count=13, Weight=1.2874\n",
            "  Class 2: Count=23, Weight=0.7277\n",
            "  Class 3: Count=21, Weight=0.7970\n",
            "  Class 4: Count=22, Weight=0.7607\n",
            "  Class 5: Count=17, Weight=0.9845\n",
            "  Class 6: Count=23, Weight=0.7277\n",
            "  Class 7: Count=18, Weight=0.9298\n",
            "  Class 8: Count=30, Weight=0.5579\n",
            "  Class 9: Count=15, Weight=1.1158\n",
            "  Class 10: Count=11, Weight=1.5215\n",
            "  Class 11: Count=12, Weight=1.3947\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.50it/s, loss=3.5, acc=8.33]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.89it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.41it/s, loss=3.08, acc=9.93]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.10it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.52it/s, loss=2.86, acc=9.82]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.04it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.57it/s, loss=2.72, acc=9.36]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.55it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.51it/s, loss=2.63, acc=8.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.08it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.48it/s, loss=2.56, acc=9.93]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.95it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.53it/s, loss=2.51, acc=10.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.06it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.53it/s, loss=2.51, acc=10.5]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.45it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.51it/s, loss=2.46, acc=12]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.09it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.47it/s, loss=2.46, acc=12.6]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.70it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.52it/s, loss=2.51, acc=12]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.02it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.45it/s, loss=2.56, acc=9.02]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.04it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.50it/s, loss=2.53, acc=11.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.65it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.52it/s, loss=2.49, acc=11.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.03it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.56it/s, loss=2.49, acc=12.6]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.40it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.52it/s, loss=2.49, acc=12]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.04it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.47it/s, loss=2.46, acc=13.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 2 Results:\n",
            "  Accuracy: 0.1461\n",
            "  Weighted F1: 0.0948\n",
            "  Training Time: 8.02 minutes\n",
            "\n",
            "======================================================================\n",
            "FOLD 3/5\n",
            "======================================================================\n",
            "\n",
            "Fold 3 - Train: 876, Val: 219\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 876\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=57, Weight=1.1759\n",
            "  Class 1: Count=49, Weight=1.3679\n",
            "  Class 2: Count=92, Weight=0.7286\n",
            "  Class 3: Count=84, Weight=0.7980\n",
            "  Class 4: Count=88, Weight=0.7617\n",
            "  Class 5: Count=71, Weight=0.9441\n",
            "  Class 6: Count=89, Weight=0.7531\n",
            "  Class 7: Count=72, Weight=0.9310\n",
            "  Class 8: Count=120, Weight=0.5586\n",
            "  Class 9: Count=60, Weight=1.1172\n",
            "  Class 10: Count=44, Weight=1.5234\n",
            "  Class 11: Count=50, Weight=1.3406\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 219\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=14, Weight=1.1850\n",
            "  Class 1: Count=12, Weight=1.3825\n",
            "  Class 2: Count=24, Weight=0.6912\n",
            "  Class 3: Count=21, Weight=0.7900\n",
            "  Class 4: Count=22, Weight=0.7541\n",
            "  Class 5: Count=17, Weight=0.9759\n",
            "  Class 6: Count=22, Weight=0.7541\n",
            "  Class 7: Count=19, Weight=0.8731\n",
            "  Class 8: Count=30, Weight=0.5530\n",
            "  Class 9: Count=15, Weight=1.1060\n",
            "  Class 10: Count=10, Weight=1.6590\n",
            "  Class 11: Count=13, Weight=1.2761\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.47it/s, loss=3.53, acc=9.82]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.84it/s]\n",
            "Training: 100%|██████████| 28/28 [00:20<00:00,  1.34it/s, loss=3.19, acc=7.99]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.91it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.52it/s, loss=2.96, acc=6.74]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.45it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.54it/s, loss=2.88, acc=8.22]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.94it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.47it/s, loss=2.68, acc=10]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.89it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.55it/s, loss=2.58, acc=10.3]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.71it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.46it/s, loss=2.54, acc=10.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.93it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.55it/s, loss=2.48, acc=13.9]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.40it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.46it/s, loss=2.48, acc=10.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.84it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.48it/s, loss=2.49, acc=11]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.74it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.57it/s, loss=2.54, acc=9.82]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.99it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.52it/s, loss=2.54, acc=10.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.87it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.54it/s, loss=2.5, acc=11.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.96it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.46it/s, loss=2.54, acc=11.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.94it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.59it/s, loss=2.5, acc=10.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.59it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.55it/s, loss=2.5, acc=11.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.90it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.58it/s, loss=2.49, acc=12.1]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.32it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.53it/s, loss=2.47, acc=14.5]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.49it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.51it/s, loss=2.45, acc=12.6]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.35it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.56it/s, loss=2.46, acc=13]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.94it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.54it/s, loss=2.43, acc=13.6]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.50it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.57it/s, loss=2.45, acc=12.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.91it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.49it/s, loss=2.39, acc=14.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.84it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.55it/s, loss=2.4, acc=15]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.89it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.42it/s, loss=2.41, acc=15.2]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.98it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.52it/s, loss=2.38, acc=16.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.46it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.60it/s, loss=2.36, acc=17.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.02it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.52it/s, loss=2.36, acc=16.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 3 Results:\n",
            "  Accuracy: 0.1918\n",
            "  Weighted F1: 0.1026\n",
            "  Training Time: 12.91 minutes\n",
            "\n",
            "======================================================================\n",
            "FOLD 4/5\n",
            "======================================================================\n",
            "\n",
            "Fold 4 - Train: 876, Val: 219\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 876\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=57, Weight=1.1732\n",
            "  Class 1: Count=49, Weight=1.3647\n",
            "  Class 2: Count=93, Weight=0.7190\n",
            "  Class 3: Count=84, Weight=0.7961\n",
            "  Class 4: Count=88, Weight=0.7599\n",
            "  Class 5: Count=70, Weight=0.9553\n",
            "  Class 6: Count=89, Weight=0.7514\n",
            "  Class 7: Count=73, Weight=0.9160\n",
            "  Class 8: Count=120, Weight=0.5573\n",
            "  Class 9: Count=60, Weight=1.1145\n",
            "  Class 10: Count=43, Weight=1.5551\n",
            "  Class 11: Count=50, Weight=1.3374\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 219\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=14, Weight=1.1976\n",
            "  Class 1: Count=12, Weight=1.3972\n",
            "  Class 2: Count=23, Weight=0.7290\n",
            "  Class 3: Count=21, Weight=0.7984\n",
            "  Class 4: Count=22, Weight=0.7621\n",
            "  Class 5: Count=18, Weight=0.9315\n",
            "  Class 6: Count=22, Weight=0.7621\n",
            "  Class 7: Count=18, Weight=0.9315\n",
            "  Class 8: Count=30, Weight=0.5589\n",
            "  Class 9: Count=15, Weight=1.1178\n",
            "  Class 10: Count=11, Weight=1.5242\n",
            "  Class 11: Count=13, Weight=1.2897\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.62it/s, loss=3.38, acc=9.59]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.22it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=3.14, acc=8.11]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.33it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.57it/s, loss=2.88, acc=9.13]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.65it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.62it/s, loss=2.71, acc=9.93]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.25it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.64it/s, loss=2.63, acc=9.82]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.20it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.61it/s, loss=2.57, acc=9.25]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.20it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.65it/s, loss=2.55, acc=11.1]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.66it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.62it/s, loss=2.5, acc=12.2]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.18it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.46, acc=13.5]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.66it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.66it/s, loss=2.45, acc=13.2]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.32it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.59it/s, loss=2.53, acc=10.6]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.68it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.51, acc=12.2]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.19it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.62it/s, loss=2.48, acc=14.5]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.63it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.5, acc=12.9]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.64it/s, loss=2.46, acc=11.9]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.63it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.57it/s, loss=2.46, acc=14.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.59it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.65it/s, loss=2.43, acc=14.3]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.60it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.65it/s, loss=2.4, acc=16.7]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.60it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.54it/s, loss=2.39, acc=15.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.64it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.59it/s, loss=2.41, acc=16.9]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.22it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.37, acc=15.3]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.67it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.64it/s, loss=2.34, acc=18.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.56it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.55it/s, loss=2.35, acc=19.1]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.50it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.61it/s, loss=2.32, acc=17.5]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.61it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.55it/s, loss=2.27, acc=21.2]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.63it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.59it/s, loss=2.26, acc=21.9]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.28it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.60it/s, loss=2.24, acc=21.7]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.64it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.57it/s, loss=2.23, acc=22.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.45it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.58it/s, loss=2.27, acc=22.1]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.19it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.61it/s, loss=2.24, acc=21.6]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.65it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.59it/s, loss=2.28, acc=20.1]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.34it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.33, acc=18.2]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.67it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.57it/s, loss=2.3, acc=18.6]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.41it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.60it/s, loss=2.3, acc=19.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.54it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.52it/s, loss=2.28, acc=19.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.63it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.59it/s, loss=2.28, acc=20.7]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.58it/s, loss=2.3, acc=18.9]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.60it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.66it/s, loss=2.3, acc=20]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 4 Results:\n",
            "  Accuracy: 0.2237\n",
            "  Weighted F1: 0.2021\n",
            "  Training Time: 16.91 minutes\n",
            "\n",
            "======================================================================\n",
            "FOLD 5/5\n",
            "======================================================================\n",
            "\n",
            "Fold 5 - Train: 876, Val: 219\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 876\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=57, Weight=1.1732\n",
            "  Class 1: Count=49, Weight=1.3647\n",
            "  Class 2: Count=93, Weight=0.7190\n",
            "  Class 3: Count=84, Weight=0.7961\n",
            "  Class 4: Count=88, Weight=0.7599\n",
            "  Class 5: Count=70, Weight=0.9553\n",
            "  Class 6: Count=89, Weight=0.7514\n",
            "  Class 7: Count=73, Weight=0.9160\n",
            "  Class 8: Count=120, Weight=0.5573\n",
            "  Class 9: Count=60, Weight=1.1145\n",
            "  Class 10: Count=43, Weight=1.5551\n",
            "  Class 11: Count=50, Weight=1.3374\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 219\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=14, Weight=1.1976\n",
            "  Class 1: Count=12, Weight=1.3972\n",
            "  Class 2: Count=23, Weight=0.7290\n",
            "  Class 3: Count=21, Weight=0.7984\n",
            "  Class 4: Count=22, Weight=0.7621\n",
            "  Class 5: Count=18, Weight=0.9315\n",
            "  Class 6: Count=22, Weight=0.7621\n",
            "  Class 7: Count=18, Weight=0.9315\n",
            "  Class 8: Count=30, Weight=0.5589\n",
            "  Class 9: Count=15, Weight=1.1178\n",
            "  Class 10: Count=11, Weight=1.5242\n",
            "  Class 11: Count=13, Weight=1.2897\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.59it/s, loss=3.57, acc=9.02]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.24it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.47it/s, loss=3.19, acc=9.36]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.41it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.54it/s, loss=3, acc=10.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.87it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.57it/s, loss=2.68, acc=5.94]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.84it/s]\n",
            "Training: 100%|██████████| 28/28 [00:19<00:00,  1.46it/s, loss=2.61, acc=9.7]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.82it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.56it/s, loss=2.53, acc=11.1]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.53it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.49it/s, loss=2.49, acc=10.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.77it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.59it/s, loss=2.51, acc=9.82]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.56it/s, loss=2.49, acc=11.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.84it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.53it/s, loss=2.46, acc=12.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.47it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.62it/s, loss=2.53, acc=9.47]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.84it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.50it/s, loss=2.56, acc=9.36]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.80it/s]\n",
            "Training: 100%|██████████| 28/28 [00:18<00:00,  1.54it/s, loss=2.51, acc=11.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 5 Results:\n",
            "  Accuracy: 0.1507\n",
            "  Weighted F1: 0.1058\n",
            "  Training Time: 5.04 minutes\n",
            "\n",
            "======================================================================\n",
            "CROSS-VALIDATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Cross-Validation Results (Mean ± Std):\n",
            "  Accuracy:     0.1845 ± 0.0312\n",
            "  Precision:    0.1282 ± 0.0531\n",
            "  Recall:       0.1845 ± 0.0312\n",
            "  Macro F1:     0.1525 ± 0.0432\n",
            "  Weighted F1:  0.1358 ± 0.0436\n",
            "  Training Time: 11.50 minutes per fold\n",
            "  Inference Time: 4.29 ms per image\n",
            "\n",
            "Per-Fold Results:\n",
            "  Fold 1: Acc=0.2100, F1=0.1737\n",
            "  Fold 2: Acc=0.1461, F1=0.0948\n",
            "  Fold 3: Acc=0.1918, F1=0.1026\n",
            "  Fold 4: Acc=0.2237, F1=0.2021\n",
            "  Fold 5: Acc=0.1507, F1=0.1058\n",
            "\n",
            "✓ Saved: exp2_vgg13_cv_optimized_cv_results.json\n",
            "✓ Saved: exp2_vgg13_cv_optimized_cv_results.png\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT 2B: RESNET18 WITH 5-FOLD CROSS-VALIDATION\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "EXP2_RESNET18_CV_OPTIMIZED: RESNET18 WITH 5-FOLD CV\n",
            "======================================================================\n",
            "\n",
            "Loading training data from: /content/drive/MyDrive/PlantDoc-Dataset-master_experiment2/train\n",
            "Loading test data from: /content/drive/MyDrive/PlantDoc-Dataset-master_experiment2/test\n",
            "\n",
            "✓ Total samples for CV: 1095\n",
            "\n",
            "======================================================================\n",
            "Complete Dataset for CV Analysis\n",
            "======================================================================\n",
            "Total images: 1095\n",
            "Number of classes: 12\n",
            "\n",
            "Class Distribution:\n",
            "  Pepper__bell___Bacterial_spot: 71 images (6.5%)\n",
            "  Pepper__bell___healthy: 61 images (5.6%)\n",
            "  Potato___Early_blight: 116 images (10.6%)\n",
            "  Potato___Late_blight: 105 images (9.6%)\n",
            "  Tomato_Bacterial_spot: 110 images (10.0%)\n",
            "  Tomato_Early_blight: 88 images (8.0%)\n",
            "  Tomato_Late_blight: 111 images (10.1%)\n",
            "  Tomato_Leaf_Mold: 91 images (8.3%)\n",
            "  Tomato_Septoria_leaf_spot: 150 images (13.7%)\n",
            "  Tomato__Tomato_YellowLeaf__Curl_Virus: 75 images (6.8%)\n",
            "  Tomato__Tomato_mosaic_virus: 54 images (4.9%)\n",
            "  Tomato_healthy: 63 images (5.8%)\n",
            "\n",
            "Imbalance Ratio: 2.78\n",
            "\n",
            "✓ Using MODERATE augmentation (RECOMMENDED)\n",
            "\n",
            "======================================================================\n",
            "STARTING 5-FOLD CROSS-VALIDATION\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "FOLD 1/5\n",
            "======================================================================\n",
            "\n",
            "Fold 1 - Train: 876, Val: 219\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 876\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=56, Weight=1.1947\n",
            "  Class 1: Count=49, Weight=1.3653\n",
            "  Class 2: Count=93, Weight=0.7194\n",
            "  Class 3: Count=84, Weight=0.7964\n",
            "  Class 4: Count=88, Weight=0.7602\n",
            "  Class 5: Count=70, Weight=0.9557\n",
            "  Class 6: Count=89, Weight=0.7517\n",
            "  Class 7: Count=73, Weight=0.9164\n",
            "  Class 8: Count=120, Weight=0.5575\n",
            "  Class 9: Count=60, Weight=1.1150\n",
            "  Class 10: Count=43, Weight=1.5558\n",
            "  Class 11: Count=51, Weight=1.3118\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 219\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=15, Weight=1.1152\n",
            "  Class 1: Count=12, Weight=1.3940\n",
            "  Class 2: Count=23, Weight=0.7273\n",
            "  Class 3: Count=21, Weight=0.7966\n",
            "  Class 4: Count=22, Weight=0.7604\n",
            "  Class 5: Count=18, Weight=0.9293\n",
            "  Class 6: Count=22, Weight=0.7604\n",
            "  Class 7: Count=18, Weight=0.9293\n",
            "  Class 8: Count=30, Weight=0.5576\n",
            "  Class 9: Count=15, Weight=1.1152\n",
            "  Class 10: Count=11, Weight=1.5207\n",
            "  Class 11: Count=12, Weight=1.3940\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.67it/s, loss=2.53, acc=8.22]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.18it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.59it/s, loss=2.48, acc=11.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.95it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.69it/s, loss=2.46, acc=13.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.19it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.71it/s, loss=2.44, acc=15]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.61it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.69it/s, loss=2.42, acc=16.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.23it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.71it/s, loss=2.41, acc=15.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.26it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.60it/s, loss=2.39, acc=17.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.27it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.37, acc=17.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.32it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.37, acc=18.6]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.60it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.34, acc=18]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.30it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.69it/s, loss=2.37, acc=19.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.77it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.37, acc=19.2]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.27it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.67it/s, loss=2.38, acc=18.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.28it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.65it/s, loss=2.39, acc=17.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.85it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.36, acc=17]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.36it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.37, acc=17.9]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.74it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.65it/s, loss=2.31, acc=18.2]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.25it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.67it/s, loss=2.31, acc=21.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.26it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.64it/s, loss=2.27, acc=22.3]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.72it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.68it/s, loss=2.28, acc=21.2]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.27it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.68it/s, loss=2.27, acc=22]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.64it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.66it/s, loss=2.24, acc=22.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.31it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.23, acc=24.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.22it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.62it/s, loss=2.19, acc=25.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.19it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.21, acc=26.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.19it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.66it/s, loss=2.16, acc=25]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.61it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.15, acc=27.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.24it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.71it/s, loss=2.16, acc=28.7]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.96it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.18, acc=26]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.27it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.69it/s, loss=2.17, acc=26.7]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.24it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.26, acc=24]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.81it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.26, acc=23.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.27it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.68it/s, loss=2.26, acc=24]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.67it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.69it/s, loss=2.23, acc=23.7]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.17it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.23, acc=26.5]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.34it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.62it/s, loss=2.22, acc=25.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.96it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.18, acc=27.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.18it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.65it/s, loss=2.19, acc=26.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.51it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.69it/s, loss=2.21, acc=25.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.23it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.68it/s, loss=2.19, acc=26.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.81it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.65it/s, loss=2.15, acc=26.6]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.34it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.11, acc=29.7]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.30it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.11, acc=29.5]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.62it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.11, acc=30.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.32it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.69it/s, loss=2.09, acc=29.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.01it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.59it/s, loss=2.1, acc=31.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1 Results:\n",
            "  Accuracy: 0.2648\n",
            "  Weighted F1: 0.2496\n",
            "  Training Time: 15.48 minutes\n",
            "\n",
            "======================================================================\n",
            "FOLD 2/5\n",
            "======================================================================\n",
            "\n",
            "Fold 2 - Train: 876, Val: 219\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 876\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=57, Weight=1.1734\n",
            "  Class 1: Count=48, Weight=1.3935\n",
            "  Class 2: Count=93, Weight=0.7192\n",
            "  Class 3: Count=84, Weight=0.7963\n",
            "  Class 4: Count=88, Weight=0.7601\n",
            "  Class 5: Count=71, Weight=0.9421\n",
            "  Class 6: Count=88, Weight=0.7601\n",
            "  Class 7: Count=73, Weight=0.9163\n",
            "  Class 8: Count=120, Weight=0.5574\n",
            "  Class 9: Count=60, Weight=1.1148\n",
            "  Class 10: Count=43, Weight=1.5555\n",
            "  Class 11: Count=51, Weight=1.3115\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 219\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=14, Weight=1.1955\n",
            "  Class 1: Count=13, Weight=1.2874\n",
            "  Class 2: Count=23, Weight=0.7277\n",
            "  Class 3: Count=21, Weight=0.7970\n",
            "  Class 4: Count=22, Weight=0.7607\n",
            "  Class 5: Count=17, Weight=0.9845\n",
            "  Class 6: Count=23, Weight=0.7277\n",
            "  Class 7: Count=18, Weight=0.9298\n",
            "  Class 8: Count=30, Weight=0.5579\n",
            "  Class 9: Count=15, Weight=1.1158\n",
            "  Class 10: Count=11, Weight=1.5215\n",
            "  Class 11: Count=12, Weight=1.3947\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.67it/s, loss=2.55, acc=9.25]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.53it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.67it/s, loss=2.51, acc=10.3]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.39it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.68it/s, loss=2.47, acc=14.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.05it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.44, acc=15.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.27it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.69it/s, loss=2.42, acc=16.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.31it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.69it/s, loss=2.41, acc=17.1]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.57it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.4, acc=16.4]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.37it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.38, acc=16.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.13it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.38, acc=17.1]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.35it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.37, acc=19.7]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.41it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.4, acc=16.6]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.52it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.4, acc=16.6]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.41it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.4, acc=16.9]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.42it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.35, acc=19.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.10it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.38, acc=18.2]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.33it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.34, acc=20.3]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.57it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.67it/s, loss=2.31, acc=18.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.33it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.66it/s, loss=2.32, acc=20.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.02it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.62it/s, loss=2.28, acc=20.9]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.43it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.71it/s, loss=2.29, acc=21.1]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.42it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.75it/s, loss=2.23, acc=24.2]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.53it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.71it/s, loss=2.25, acc=24.1]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.41it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.71it/s, loss=2.22, acc=24.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.25it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.21, acc=23.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.17it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.21, acc=25.6]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.38it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.17, acc=26.1]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.56it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.2, acc=26.4]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.35it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.71it/s, loss=2.2, acc=27.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.26it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.60it/s, loss=2.16, acc=27.1]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.41it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.15, acc=29.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.24it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.69it/s, loss=2.27, acc=24.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.67it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.27, acc=21.9]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.40it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.71it/s, loss=2.23, acc=24.5]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.41it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.66it/s, loss=2.23, acc=23.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.80it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.25, acc=23.1]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.45it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.22, acc=24.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.01it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.61it/s, loss=2.23, acc=24.8]\n",
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val: 100%|██████████| 7/7 [00:02<00:00,  2.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 2 Results:\n",
            "  Accuracy: 0.2603\n",
            "  Weighted F1: 0.2321\n",
            "  Training Time: 12.30 minutes\n",
            "\n",
            "======================================================================\n",
            "FOLD 3/5\n",
            "======================================================================\n",
            "\n",
            "Fold 3 - Train: 876, Val: 219\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 876\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=57, Weight=1.1759\n",
            "  Class 1: Count=49, Weight=1.3679\n",
            "  Class 2: Count=92, Weight=0.7286\n",
            "  Class 3: Count=84, Weight=0.7980\n",
            "  Class 4: Count=88, Weight=0.7617\n",
            "  Class 5: Count=71, Weight=0.9441\n",
            "  Class 6: Count=89, Weight=0.7531\n",
            "  Class 7: Count=72, Weight=0.9310\n",
            "  Class 8: Count=120, Weight=0.5586\n",
            "  Class 9: Count=60, Weight=1.1172\n",
            "  Class 10: Count=44, Weight=1.5234\n",
            "  Class 11: Count=50, Weight=1.3406\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 219\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=14, Weight=1.1850\n",
            "  Class 1: Count=12, Weight=1.3825\n",
            "  Class 2: Count=24, Weight=0.6912\n",
            "  Class 3: Count=21, Weight=0.7900\n",
            "  Class 4: Count=22, Weight=0.7541\n",
            "  Class 5: Count=17, Weight=0.9759\n",
            "  Class 6: Count=22, Weight=0.7541\n",
            "  Class 7: Count=19, Weight=0.8731\n",
            "  Class 8: Count=30, Weight=0.5530\n",
            "  Class 9: Count=15, Weight=1.1060\n",
            "  Class 10: Count=10, Weight=1.6590\n",
            "  Class 11: Count=13, Weight=1.2761\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.69it/s, loss=2.57, acc=7.42]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.39it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.49, acc=12.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.08it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.47, acc=11.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.97it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.64it/s, loss=2.43, acc=16]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.17it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.41, acc=14.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.16it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.4, acc=17.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.50it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.39, acc=17.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.16it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.35, acc=18.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.10it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.66it/s, loss=2.35, acc=19.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.93it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.36, acc=18.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.20it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.77it/s, loss=2.36, acc=16.6]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.54it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.36, acc=18.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.20it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.36, acc=20.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.20it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.36, acc=18.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.83it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.33, acc=18.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.21it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.31, acc=21.2]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.50it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.3, acc=20.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.22it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.29, acc=21.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.22it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.66it/s, loss=2.3, acc=23.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.80it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.77it/s, loss=2.28, acc=22.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.11it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.75it/s, loss=2.27, acc=23.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.77it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.66it/s, loss=2.24, acc=24.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.21it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.2, acc=28.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.21it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.24, acc=22.9]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.46it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.15, acc=27.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.14it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.17, acc=27.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.07it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.61it/s, loss=2.18, acc=26.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.22it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.75it/s, loss=2.19, acc=25.7]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.11it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.18, acc=27.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.46it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.71it/s, loss=2.16, acc=25.2]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.12it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.27, acc=23.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.94it/s]\n",
            "Training: 100%|██████████| 28/28 [00:17<00:00,  1.63it/s, loss=2.29, acc=25.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.20it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.28, acc=22.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 3 Results:\n",
            "  Accuracy: 0.2603\n",
            "  Weighted F1: 0.2130\n",
            "  Training Time: 10.96 minutes\n",
            "\n",
            "======================================================================\n",
            "FOLD 4/5\n",
            "======================================================================\n",
            "\n",
            "Fold 4 - Train: 876, Val: 219\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 876\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=57, Weight=1.1732\n",
            "  Class 1: Count=49, Weight=1.3647\n",
            "  Class 2: Count=93, Weight=0.7190\n",
            "  Class 3: Count=84, Weight=0.7961\n",
            "  Class 4: Count=88, Weight=0.7599\n",
            "  Class 5: Count=70, Weight=0.9553\n",
            "  Class 6: Count=89, Weight=0.7514\n",
            "  Class 7: Count=73, Weight=0.9160\n",
            "  Class 8: Count=120, Weight=0.5573\n",
            "  Class 9: Count=60, Weight=1.1145\n",
            "  Class 10: Count=43, Weight=1.5551\n",
            "  Class 11: Count=50, Weight=1.3374\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 219\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=14, Weight=1.1976\n",
            "  Class 1: Count=12, Weight=1.3972\n",
            "  Class 2: Count=23, Weight=0.7290\n",
            "  Class 3: Count=21, Weight=0.7984\n",
            "  Class 4: Count=22, Weight=0.7621\n",
            "  Class 5: Count=18, Weight=0.9315\n",
            "  Class 6: Count=22, Weight=0.7621\n",
            "  Class 7: Count=18, Weight=0.9315\n",
            "  Class 8: Count=30, Weight=0.5589\n",
            "  Class 9: Count=15, Weight=1.1178\n",
            "  Class 10: Count=11, Weight=1.5242\n",
            "  Class 11: Count=13, Weight=1.2897\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.53, acc=10.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.68it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.86it/s, loss=2.49, acc=13.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.77it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.82it/s, loss=2.46, acc=13.2]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.28it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.80it/s, loss=2.43, acc=12.9]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.74it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.84it/s, loss=2.42, acc=15]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.59it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.39, acc=15.5]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.73it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.84it/s, loss=2.36, acc=18.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.74it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.81it/s, loss=2.35, acc=18.8]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.25it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.78it/s, loss=2.37, acc=17.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.76it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.82it/s, loss=2.37, acc=17.7]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.43it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.78it/s, loss=2.35, acc=19.1]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.66it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.83it/s, loss=2.4, acc=18.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.79it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.85it/s, loss=2.4, acc=17]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.27it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.82it/s, loss=2.36, acc=17.2]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.78it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.83it/s, loss=2.37, acc=18.6]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.70it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.32, acc=19.9]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.47it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.83it/s, loss=2.32, acc=19.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.80it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.82it/s, loss=2.26, acc=23.1]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.31it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.82it/s, loss=2.26, acc=22.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.82it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.83it/s, loss=2.27, acc=22]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.81it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.81it/s, loss=2.25, acc=21.2]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.81it/s, loss=2.23, acc=23.9]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.74it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.80it/s, loss=2.19, acc=26.6]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.78it/s, loss=2.19, acc=26.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.77it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.83it/s, loss=2.17, acc=26.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.75it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.17, acc=26.6]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.32it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.82it/s, loss=2.16, acc=28.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.73it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.82it/s, loss=2.15, acc=27.7]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.39it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.13, acc=29.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.77it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.83it/s, loss=2.15, acc=26.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.80it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.85it/s, loss=2.28, acc=23.6]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.31it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.86it/s, loss=2.26, acc=23.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.76it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.83it/s, loss=2.24, acc=23.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.46it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.78it/s, loss=2.2, acc=25.1]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.66it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.81it/s, loss=2.22, acc=24.7]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.80it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.83it/s, loss=2.2, acc=28]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.27it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.83it/s, loss=2.18, acc=26.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.78it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.84it/s, loss=2.19, acc=26.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.79it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.84it/s, loss=2.16, acc=27.6]\n",
            "Val: 100%|██████████| 7/7 [00:05<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 4 Results:\n",
            "  Accuracy: 0.2557\n",
            "  Weighted F1: 0.2305\n",
            "  Training Time: 12.95 minutes\n",
            "\n",
            "======================================================================\n",
            "FOLD 5/5\n",
            "======================================================================\n",
            "\n",
            "Fold 5 - Train: 876, Val: 219\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 876\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=57, Weight=1.1732\n",
            "  Class 1: Count=49, Weight=1.3647\n",
            "  Class 2: Count=93, Weight=0.7190\n",
            "  Class 3: Count=84, Weight=0.7961\n",
            "  Class 4: Count=88, Weight=0.7599\n",
            "  Class 5: Count=70, Weight=0.9553\n",
            "  Class 6: Count=89, Weight=0.7514\n",
            "  Class 7: Count=73, Weight=0.9160\n",
            "  Class 8: Count=120, Weight=0.5573\n",
            "  Class 9: Count=60, Weight=1.1145\n",
            "  Class 10: Count=43, Weight=1.5551\n",
            "  Class 11: Count=50, Weight=1.3374\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CLASS WEIGHTS FOR LOSS FUNCTION\n",
            "======================================================================\n",
            "Total Samples: 219\n",
            "Number of Classes: 12\n",
            "Calculated Class Weights (Inverse Frequency):\n",
            "  Class 0: Count=14, Weight=1.1976\n",
            "  Class 1: Count=12, Weight=1.3972\n",
            "  Class 2: Count=23, Weight=0.7290\n",
            "  Class 3: Count=21, Weight=0.7984\n",
            "  Class 4: Count=22, Weight=0.7621\n",
            "  Class 5: Count=18, Weight=0.9315\n",
            "  Class 6: Count=22, Weight=0.7621\n",
            "  Class 7: Count=18, Weight=0.9315\n",
            "  Class 8: Count=30, Weight=0.5589\n",
            "  Class 9: Count=15, Weight=1.1178\n",
            "  Class 10: Count=11, Weight=1.5242\n",
            "  Class 11: Count=13, Weight=1.2897\n",
            "✓ Class weights will be used in CrossEntropyLoss\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.75it/s, loss=2.54, acc=8.79]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.63it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.66it/s, loss=2.49, acc=10.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.05it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.71it/s, loss=2.48, acc=11.2]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.04it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.49, acc=10.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.49it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.44, acc=13.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.03it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.42, acc=13.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.54it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.42, acc=15.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.04it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.37, acc=17.4]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.06it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.65it/s, loss=2.38, acc=16.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.75it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.77it/s, loss=2.38, acc=16.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.03it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.41, acc=13.8]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.54it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.44, acc=15.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.06it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.77it/s, loss=2.42, acc=16]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.06it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.38, acc=16.2]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.63it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.39, acc=17.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.08it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.41, acc=16.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.73it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.67it/s, loss=2.37, acc=18]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.06it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.34, acc=20.7]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.05it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.29, acc=20.1]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.52it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.3, acc=22.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.99it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.25, acc=23.3]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.72it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.28, acc=21.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.08it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.27, acc=22.3]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.05it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.22, acc=26.7]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.42it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.2, acc=26]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  1.99it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.22, acc=25.3]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.70it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.66it/s, loss=2.19, acc=26.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.03it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.2, acc=25.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.03it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.67it/s, loss=2.16, acc=27.1]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.58it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.71it/s, loss=2.17, acc=25.1]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.02it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.26, acc=23.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.55it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.29, acc=22.8]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.03it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.75it/s, loss=2.28, acc=22.9]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.06it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.70it/s, loss=2.28, acc=21.7]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.57it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.72it/s, loss=2.28, acc=23.2]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.07it/s]\n",
            "Training: 100%|██████████| 28/28 [00:15<00:00,  1.76it/s, loss=2.24, acc=23.4]\n",
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.61it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.73it/s, loss=2.22, acc=24.5]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.06it/s]\n",
            "Training: 100%|██████████| 28/28 [00:16<00:00,  1.74it/s, loss=2.21, acc=26.6]\n",
            "Val: 100%|██████████| 7/7 [00:03<00:00,  2.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val: 100%|██████████| 7/7 [00:04<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 5 Results:\n",
            "  Accuracy: 0.2922\n",
            "  Weighted F1: 0.2647\n",
            "  Training Time: 12.71 minutes\n",
            "\n",
            "======================================================================\n",
            "CROSS-VALIDATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Cross-Validation Results (Mean ± Std):\n",
            "  Accuracy:     0.2667 ± 0.0131\n",
            "  Precision:    0.2753 ± 0.0396\n",
            "  Recall:       0.2667 ± 0.0131\n",
            "  Macro F1:     0.2572 ± 0.0192\n",
            "  Weighted F1:  0.2380 ± 0.0177\n",
            "  Training Time: 12.88 minutes per fold\n",
            "  Inference Time: 1.04 ms per image\n",
            "\n",
            "Per-Fold Results:\n",
            "  Fold 1: Acc=0.2648, F1=0.2496\n",
            "  Fold 2: Acc=0.2603, F1=0.2321\n",
            "  Fold 3: Acc=0.2603, F1=0.2130\n",
            "  Fold 4: Acc=0.2557, F1=0.2305\n",
            "  Fold 5: Acc=0.2922, F1=0.2647\n",
            "\n",
            "✓ Saved: exp2_resnet18_cv_optimized_cv_results.json\n",
            "✓ Saved: exp2_resnet18_cv_optimized_cv_results.png\n",
            "\n",
            "======================================================================\n",
            "ALL EXPERIMENTS COMPARISON\n",
            "======================================================================\n",
            "\n",
            "              Metric Exp1 (VGG13 Large) Exp2 (VGG13 CV) Exp2 (ResNet18 CV)\n",
            "           Accuracy             0.1863 0.1845 ± 0.0312    0.2667 ± 0.0131\n",
            "          Precision             0.1995 0.1282 ± 0.0531    0.2753 ± 0.0396\n",
            "             Recall             0.1863 0.1845 ± 0.0312    0.2667 ± 0.0131\n",
            "           Macro F1             0.1408 0.1525 ± 0.0432    0.2572 ± 0.0192\n",
            "        Weighted F1             0.1436 0.1358 ± 0.0436    0.2380 ± 0.0177\n",
            "Training Time (min)             113.52           11.50              12.88\n",
            "Inference Time (ms)               4.32            4.29               1.04\n",
            "\n",
            "✓ Saved: all_experiments_comparison_optimized.csv\n",
            "✓ Saved: all_experiments_comparison_optimized.png\n",
            "\n",
            "======================================================================\n",
            "🏆 BEST MODEL FOR EXPERIMENT 2 (CV): ResNet18\n",
            "======================================================================\n",
            "  Accuracy:     0.2667 ± 0.0131\n",
            "  Weighted F1:  0.2380 ± 0.0177\n",
            "  Training Time: 12.88 minutes per fold\n",
            "  Inference Time: 1.04 ms per image\n",
            "\n",
            "======================================================================\n",
            "OPTIMIZATION SUMMARY\n",
            "======================================================================\n",
            "✓ Removed WeightedRandomSampler → 3-5x faster data loading\n",
            "✓ Increased batch sizes → Fewer iterations per epoch\n",
            "✓ Persistent workers → No worker restart between epochs\n",
            "✓ Moderate augmentation → 2-3x faster augmentation pipeline\n",
            "✓ Expected speedup: 5-10x faster training (78min → 8-15min per epoch)\n",
            "\n",
            "✓ Class imbalance still handled via weighted CrossEntropyLoss\n",
            "✓ Model performance should be similar or better\n",
            "✓ Memory usage reduced\n",
            "\n",
            "======================================================================\n",
            "KEY FEATURES\n",
            "======================================================================\n",
            "✓ Experiment 1: Single train/val/test split with moderate augmentation\n",
            "✓ Experiment 2: 5-Fold Cross-Validation for robust evaluation\n",
            "✓ Inverse frequency class weights in loss function\n",
            "✓ Early stopping prevents overfitting\n",
            "✓ Mixed precision training (AMP)\n",
            "✓ Gradient clipping for stability\n",
            "✓ Cosine annealing learning rate scheduler\n",
            "\n",
            "======================================================================\n",
            "ALL EXPERIMENTS COMPLETED!\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
